{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Fundamentals I - Introduction to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python - Working with RDD operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Analyzing a log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logFile = sc.textFile(\"/home/jane/Downloads/LabData/notebook.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0-preview2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(logFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[I 12:09:13.491 NotebookApp] Using MathJax: /static/vendor/MathJax-2.5-latest/MathJax.js',\n",
       " \"[I 12:09:13.494 NotebookApp] Using existing profile dir: u'/home/notebook/.ipython/profile_default'\",\n",
       " '[I 12:09:13.513 NotebookApp] Writing notebook server cookie secret to /home/notebook/.ipython/profile_default/security/notebook_cookie_secret',\n",
       " '[C 12:09:13.565 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.',\n",
       " '[C 12:09:13.565 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using authentication. This is highly insecure and not recommended.',\n",
       " '[I 12:09:13.586 NotebookApp] Serving notebooks from local directory: /resources',\n",
       " '[I 12:09:13.586 NotebookApp] 0 active kernels ',\n",
       " '[I 12:09:13.586 NotebookApp] The IPython Notebook is running at: http://[all ip addresses on your system]:8888/',\n",
       " '[I 12:09:13.586 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).',\n",
       " '[W 12:09:13.586 NotebookApp] No web browser found: could not locate runnable browser.',\n",
       " '[I 05:23:31.876 NotebookApp] Using MathJax: /static/vendor/MathJax-2.5-latest/MathJax.js',\n",
       " \"[I 05:23:31.878 NotebookApp] Using existing profile dir: u'/home/notebook/.ipython/profile_default'\",\n",
       " '[C 05:23:31.934 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.',\n",
       " '[C 05:23:31.935 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using authentication. This is highly insecure and not recommended.',\n",
       " '[I 05:23:31.950 NotebookApp] Serving notebooks from local directory: /resources',\n",
       " '[I 05:23:31.950 NotebookApp] 0 active kernels ',\n",
       " '[I 05:23:31.951 NotebookApp] The IPython Notebook is running at: http://[all ip addresses on your system]:8888/',\n",
       " '[I 05:23:31.951 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).',\n",
       " '[W 05:23:31.951 NotebookApp] No web browser found: could not locate runnable browser.',\n",
       " '[I 17:47:53.549 NotebookApp] Using MathJax: /static/vendor/MathJax-2.5-latest/MathJax.js',\n",
       " \"[I 17:47:53.551 NotebookApp] Using existing profile dir: u'/home/notebook/.ipython/profile_default'\",\n",
       " '[C 17:47:53.607 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.',\n",
       " '[C 17:47:53.607 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using authentication. This is highly insecure and not recommended.',\n",
       " '[I 17:47:53.624 NotebookApp] Serving notebooks from local directory: /resources',\n",
       " '[I 17:47:53.625 NotebookApp] 0 active kernels ',\n",
       " '[I 17:47:53.625 NotebookApp] The IPython Notebook is running at: http://[all ip addresses on your system]:8888/',\n",
       " '[I 17:47:53.625 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).',\n",
       " '[W 17:47:53.625 NotebookApp] No web browser found: could not locate runnable browser.',\n",
       " '[I 15:44:00.376 NotebookApp] Using MathJax: /static/vendor/MathJax-2.5-latest/MathJax.js',\n",
       " \"[I 15:44:00.379 NotebookApp] Using existing profile dir: u'/home/notebook/.ipython/profile_default'\",\n",
       " '[C 15:44:00.432 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.',\n",
       " '[C 15:44:00.432 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using authentication. This is highly insecure and not recommended.',\n",
       " '[I 15:44:00.448 NotebookApp] Serving notebooks from local directory: /resources',\n",
       " '[I 15:44:00.448 NotebookApp] 0 active kernels ',\n",
       " '[I 15:44:00.448 NotebookApp] The IPython Notebook is running at: http://[all ip addresses on your system]:8888/',\n",
       " '[I 15:44:00.448 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).',\n",
       " '[W 15:44:00.449 NotebookApp] No web browser found: could not locate runnable browser.',\n",
       " '[I 14:29:17.513 NotebookApp] Writing notebook-signing key to /home/notebook/.ipython/profile_default/security/notebook_secret',\n",
       " '[W 14:29:17.514 NotebookApp] Notebook Tutorial #1 - Get Data0.ipynb is not trusted',\n",
       " '[I 14:29:18.119 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " '[I 14:29:18.121 NotebookApp] Provisioning local kernel: python2',\n",
       " '[I 14:29:18.511 NotebookApp] Kernel started: 12a23d72-5ba3-498c-88f5-bc362c8c5357',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/14 14:29:21 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/14 14:29:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/14 14:29:22 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/14 14:29:22 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/14 14:29:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/14 14:29:23 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/14 14:29:23 INFO Remoting: Starting remoting',\n",
       " '15/10/14 14:29:23 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:53333]',\n",
       " \"15/10/14 14:29:23 INFO Utils: Successfully started service 'sparkDriver' on port 53333.\",\n",
       " '15/10/14 14:29:23 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/14 14:29:23 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/14 14:29:23 INFO DiskBlockManager: Created local directory at /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6/blockmgr-c142f2f1-ebb6-4612-945b-0a67d156230a',\n",
       " '15/10/14 14:29:23 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/14 14:29:23 INFO HttpFileServer: HTTP File server directory is /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6/httpd-ed3f4ab0-7218-48bc-9d8a-3981b1cfe574',\n",
       " '15/10/14 14:29:23 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/14 14:29:24 INFO Utils: Successfully started service 'HTTP file server' on port 49932.\",\n",
       " '15/10/14 14:29:24 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/14 14:29:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.\",\n",
       " '15/10/14 14:29:24 INFO SparkUI: Started SparkUI at http://172.17.0.22:4040',\n",
       " '15/10/14 14:29:24 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/14 14:29:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35726.\",\n",
       " '15/10/14 14:29:24 INFO NettyBlockTransferService: Server created on 35726',\n",
       " '15/10/14 14:29:24 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/14 14:29:24 INFO BlockManagerMasterEndpoint: Registering block manager localhost:35726 with 265.4 MB RAM, BlockManagerId(driver, localhost, 35726)',\n",
       " '15/10/14 14:29:24 INFO BlockManagerMaster: Registered BlockManager',\n",
       " '[W 14:29:34.393 NotebookApp] Notebook Tutorial #1 - Get Data0.ipynb is not trusted',\n",
       " '[I 14:29:34.445 NotebookApp] Saving file at /Tutorial #1 - Get Data0.ipynb',\n",
       " '[W 15:33:37.041 NotebookApp] Notebook presentation.ipynb is not trusted',\n",
       " '[I 15:33:37.580 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=None, path=\"\", **kwargs={}',\n",
       " '[I 15:33:37.580 NotebookApp] Provisioning local kernel: None',\n",
       " '[I 15:33:37.609 NotebookApp] Kernel started: 12cbbe53-badb-4eee-88e0-08f04221d2be',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/15 15:33:40 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/15 15:33:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/15 15:33:41 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/15 15:33:41 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/15 15:33:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/15 15:33:42 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/15 15:33:42 INFO Remoting: Starting remoting',\n",
       " '15/10/15 15:33:42 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:47412]',\n",
       " \"15/10/15 15:33:42 INFO Utils: Successfully started service 'sparkDriver' on port 47412.\",\n",
       " '15/10/15 15:33:42 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/15 15:33:42 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/15 15:33:42 INFO DiskBlockManager: Created local directory at /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46/blockmgr-aad4e583-6a6c-479a-b021-a7e0390ea261',\n",
       " '15/10/15 15:33:42 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/15 15:33:42 INFO HttpFileServer: HTTP File server directory is /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46/httpd-80730048-1dcb-4da2-8458-8bf3eba96046',\n",
       " '15/10/15 15:33:42 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/15 15:33:42 INFO Utils: Successfully started service 'HTTP file server' on port 33688.\",\n",
       " '15/10/15 15:33:42 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/15 15:33:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
       " \"15/10/15 15:33:43 INFO Utils: Successfully started service 'SparkUI' on port 4041.\",\n",
       " '15/10/15 15:33:43 INFO SparkUI: Started SparkUI at http://172.17.0.22:4041',\n",
       " '15/10/15 15:33:43 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/15 15:33:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47915.\",\n",
       " '15/10/15 15:33:43 INFO NettyBlockTransferService: Server created on 47915',\n",
       " '15/10/15 15:33:43 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/15 15:33:43 INFO BlockManagerMasterEndpoint: Registering block manager localhost:47915 with 265.4 MB RAM, BlockManagerId(driver, localhost, 47915)',\n",
       " '15/10/15 15:33:43 INFO BlockManagerMaster: Registered BlockManager',\n",
       " '[I 15:37:39.136 NotebookApp] Saving file at /presentation.ipynb',\n",
       " '[W 15:37:39.136 NotebookApp] Saving untrusted notebook presentation.ipynb',\n",
       " '[I 16:01:55.926 NotebookApp] Saving file at /presentation.ipynb',\n",
       " '[W 16:01:55.926 NotebookApp] Saving untrusted notebook presentation.ipynb',\n",
       " '[I 16:03:58.884 NotebookApp] Saving file at /presentation.ipynb',\n",
       " '[W 16:03:58.884 NotebookApp] Saving untrusted notebook presentation.ipynb',\n",
       " '[I 16:16:06.482 NotebookApp] Saving file at /presentation.ipynb',\n",
       " '[W 16:16:06.482 NotebookApp] Saving untrusted notebook presentation.ipynb',\n",
       " '[I 16:17:08.097 NotebookApp] Kernel interrupted: 12cbbe53-badb-4eee-88e0-08f04221d2be',\n",
       " '[I 16:17:09.506 NotebookApp] Kernel interrupted: 12cbbe53-badb-4eee-88e0-08f04221d2be',\n",
       " '[I 16:17:13.095 NotebookApp] Kernel interrupted: 12cbbe53-badb-4eee-88e0-08f04221d2be',\n",
       " '[I 16:18:06.310 NotebookApp] Saving file at /presentation.ipynb',\n",
       " '[W 16:18:06.310 NotebookApp] Saving untrusted notebook presentation.ipynb',\n",
       " '[I 16:20:06.420 NotebookApp] Saving file at /presentation.ipynb',\n",
       " '[W 16:20:06.420 NotebookApp] Saving untrusted notebook presentation.ipynb',\n",
       " '[I 16:22:06.414 NotebookApp] Saving file at /presentation.ipynb',\n",
       " '[W 16:22:06.415 NotebookApp] Saving untrusted notebook presentation.ipynb',\n",
       " '[I 16:24:06.399 NotebookApp] Saving file at /presentation.ipynb',\n",
       " '[W 16:24:06.399 NotebookApp] Saving untrusted notebook presentation.ipynb',\n",
       " '[W 16:31:31.999 NotebookApp] WebSocket ping timeout after 119949 ms.',\n",
       " '[W 17:39:02.460 NotebookApp] Notebook presentation.ipynb is not trusted',\n",
       " '[W 17:39:02.972 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 17:39:02.974 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 17:40:32.344 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 17:40:32.345 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 17:50:59.807 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 17:50:59.809 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 21:46:29.786 NotebookApp] WebSocket ping timeout after 119982 ms.',\n",
       " '[W 23:57:57.893 NotebookApp] WebSocket ping timeout after 90000 ms.',\n",
       " '[W 00:24:31.376 NotebookApp] WebSocket ping timeout after 90000 ms.',\n",
       " '[W 01:13:03.731 NotebookApp] WebSocket ping timeout after 119946 ms.',\n",
       " '[W 02:43:09.449 NotebookApp] WebSocket ping timeout after 90001 ms.',\n",
       " '[W 03:44:53.227 NotebookApp] WebSocket ping timeout after 119924 ms.',\n",
       " '[W 09:48:40.685 NotebookApp] WebSocket ping timeout after 119918 ms.',\n",
       " '[W 13:08:17.861 NotebookApp] Notebook Untitled.ipynb is not trusted',\n",
       " '[I 13:08:17.970 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " '[I 13:08:17.970 NotebookApp] Provisioning local kernel: python2',\n",
       " '[I 13:08:18.000 NotebookApp] Kernel started: fba3d4f2-e87c-461e-b434-374434e25aac',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/16 13:08:21 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/16 13:08:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/16 13:08:21 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/16 13:08:21 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/16 13:08:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/16 13:08:22 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/16 13:08:22 INFO Remoting: Starting remoting',\n",
       " '15/10/16 13:08:23 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:58378]',\n",
       " \"15/10/16 13:08:23 INFO Utils: Successfully started service 'sparkDriver' on port 58378.\",\n",
       " '15/10/16 13:08:23 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/16 13:08:23 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/16 13:08:23 INFO DiskBlockManager: Created local directory at /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/blockmgr-24ee1e5a-9311-4665-8ce7-56fc1f0601a0',\n",
       " '15/10/16 13:08:23 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/16 13:08:23 INFO HttpFileServer: HTTP File server directory is /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/httpd-98632027-ee06-401b-a027-b7973b158023',\n",
       " '15/10/16 13:08:23 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/16 13:08:23 INFO Utils: Successfully started service 'HTTP file server' on port 58885.\",\n",
       " '15/10/16 13:08:23 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/16 13:08:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
       " \"15/10/16 13:08:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\",\n",
       " \"15/10/16 13:08:23 INFO Utils: Successfully started service 'SparkUI' on port 4042.\",\n",
       " '15/10/16 13:08:23 INFO SparkUI: Started SparkUI at http://172.17.0.22:4042',\n",
       " '15/10/16 13:08:23 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/16 13:08:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34420.\",\n",
       " '15/10/16 13:08:23 INFO NettyBlockTransferService: Server created on 34420',\n",
       " '15/10/16 13:08:23 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/16 13:08:23 INFO BlockManagerMasterEndpoint: Registering block manager localhost:34420 with 265.4 MB RAM, BlockManagerId(driver, localhost, 34420)',\n",
       " '15/10/16 13:08:23 INFO BlockManagerMaster: Registered BlockManager',\n",
       " '[I 13:10:18.237 NotebookApp] Saving file at /Untitled.ipynb',\n",
       " '[I 13:12:18.438 NotebookApp] Saving file at /Untitled.ipynb',\n",
       " '[I 13:13:22.038 NotebookApp] Request restart_kernel: fba3d4f2-e87c-461e-b434-374434e25aac, <IPython.kernel.ioloop.manager.IOLoopKernelManager object at 0x7fd6315aea50>',\n",
       " '15/10/16 13:13:22 INFO SparkUI: Stopped Spark web UI at http://172.17.0.22:4042',\n",
       " '15/10/16 13:13:22 INFO DAGScheduler: Stopping DAGScheduler',\n",
       " '15/10/16 13:13:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!',\n",
       " '15/10/16 13:13:22 INFO Utils: path = /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/blockmgr-24ee1e5a-9311-4665-8ce7-56fc1f0601a0, already present as root for deletion.',\n",
       " '15/10/16 13:13:22 INFO MemoryStore: MemoryStore cleared',\n",
       " '15/10/16 13:13:22 INFO BlockManager: BlockManager stopped',\n",
       " '15/10/16 13:13:22 INFO BlockManagerMaster: BlockManagerMaster stopped',\n",
       " '15/10/16 13:13:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!',\n",
       " '15/10/16 13:13:22 INFO SparkContext: Successfully stopped SparkContext',\n",
       " '15/10/16 13:13:22 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.',\n",
       " '15/10/16 13:13:22 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.',\n",
       " '15/10/16 13:13:22 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.',\n",
       " '15/10/16 13:13:22 INFO Utils: Shutdown hook called',\n",
       " '15/10/16 13:13:22 INFO Utils: Deleting directory /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/pyspark-2107622a-f8ad-4b5a-b456-f0e414fbed40',\n",
       " '15/10/16 13:13:22 INFO Utils: Deleting directory /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70',\n",
       " '[I 13:13:22.762 NotebookApp] Kernel restarted: fba3d4f2-e87c-461e-b434-374434e25aac',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/16 13:13:25 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/16 13:13:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/16 13:13:26 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/16 13:13:26 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/16 13:13:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/16 13:13:27 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/16 13:13:27 INFO Remoting: Starting remoting',\n",
       " '15/10/16 13:13:27 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:38668]',\n",
       " \"15/10/16 13:13:27 INFO Utils: Successfully started service 'sparkDriver' on port 38668.\",\n",
       " '15/10/16 13:13:27 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/16 13:13:27 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/16 13:13:27 INFO DiskBlockManager: Created local directory at /tmp/spark-52bdb6b1-7781-4abd-9758-bfc0d2a578ec/blockmgr-e0992345-e860-44ea-aaca-50e75bd99684',\n",
       " '15/10/16 13:13:27 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/16 13:13:27 INFO HttpFileServer: HTTP File server directory is /tmp/spark-52bdb6b1-7781-4abd-9758-bfc0d2a578ec/httpd-fe5e1d28-9663-460b-97fd-e2374b912583',\n",
       " '15/10/16 13:13:27 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/16 13:13:28 INFO Utils: Successfully started service 'HTTP file server' on port 36908.\",\n",
       " '15/10/16 13:13:28 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/16 13:13:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
       " \"15/10/16 13:13:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\",\n",
       " \"15/10/16 13:13:28 INFO Utils: Successfully started service 'SparkUI' on port 4042.\",\n",
       " '15/10/16 13:13:28 INFO SparkUI: Started SparkUI at http://172.17.0.22:4042',\n",
       " '15/10/16 13:13:28 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/16 13:13:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44407.\",\n",
       " '15/10/16 13:13:28 INFO NettyBlockTransferService: Server created on 44407',\n",
       " '15/10/16 13:13:28 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/16 13:13:28 INFO BlockManagerMasterEndpoint: Registering block manager localhost:44407 with 265.4 MB RAM, BlockManagerId(driver, localhost, 44407)',\n",
       " '15/10/16 13:13:28 INFO BlockManagerMaster: Registered BlockManager',\n",
       " '[I 13:14:18.299 NotebookApp] Saving file at /Untitled.ipynb',\n",
       " '15/10/16 13:15:35 INFO HiveContext: Initializing execution hive, version 0.13.1',\n",
       " '15/10/16 13:15:35 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore',\n",
       " '15/10/16 13:15:35 INFO ObjectStore: ObjectStore, initialize called',\n",
       " '15/10/16 13:15:35 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored',\n",
       " '15/10/16 13:15:35 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored',\n",
       " '15/10/16 13:15:35 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)',\n",
       " '15/10/16 13:15:35 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)',\n",
       " '15/10/16 13:15:37 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"',\n",
       " '15/10/16 13:15:37 INFO MetaStoreDirectSql: MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: \"@\" (64), after : \"\".',\n",
       " '15/10/16 13:15:38 INFO Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.',\n",
       " '15/10/16 13:15:38 INFO Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.',\n",
       " '15/10/16 13:15:39 INFO Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.',\n",
       " '15/10/16 13:15:39 INFO Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.',\n",
       " '15/10/16 13:15:39 INFO ObjectStore: Initialized ObjectStore',\n",
       " '15/10/16 13:15:39 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa',\n",
       " '15/10/16 13:15:40 INFO HiveMetaStore: Added admin role in metastore',\n",
       " '15/10/16 13:15:40 INFO HiveMetaStore: Added public role in metastore',\n",
       " '15/10/16 13:15:40 INFO HiveMetaStore: No user is added in admin role, since config is empty',\n",
       " '15/10/16 13:15:40 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.',\n",
       " '15/10/16 13:15:41 INFO HiveContext: Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.',\n",
       " '15/10/16 13:15:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/16 13:15:41 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore',\n",
       " '15/10/16 13:15:41 INFO ObjectStore: ObjectStore, initialize called',\n",
       " '15/10/16 13:15:42 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored',\n",
       " '15/10/16 13:15:42 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored',\n",
       " '15/10/16 13:15:42 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)',\n",
       " '15/10/16 13:15:42 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)',\n",
       " '15/10/16 13:15:43 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"',\n",
       " '15/10/16 13:15:43 INFO MetaStoreDirectSql: MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: \"@\" (64), after : \"\".',\n",
       " '15/10/16 13:15:44 INFO Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.',\n",
       " '15/10/16 13:15:44 INFO Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.',\n",
       " '15/10/16 13:15:45 INFO Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.',\n",
       " '15/10/16 13:15:45 INFO Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.',\n",
       " '15/10/16 13:15:46 INFO ObjectStore: Initialized ObjectStore',\n",
       " '15/10/16 13:15:46 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa',\n",
       " '15/10/16 13:15:46 INFO HiveMetaStore: Added admin role in metastore',\n",
       " '15/10/16 13:15:46 INFO HiveMetaStore: Added public role in metastore',\n",
       " '15/10/16 13:15:46 INFO HiveMetaStore: No user is added in admin role, since config is empty',\n",
       " '15/10/16 13:15:46 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.',\n",
       " '15/10/16 13:15:47 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:-2',\n",
       " '15/10/16 13:15:47 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:-2) with 1 output partitions (allowLocal=false)',\n",
       " '15/10/16 13:15:47 INFO DAGScheduler: Final stage: ResultStage 0(showString at NativeMethodAccessorImpl.java:-2)',\n",
       " '15/10/16 13:15:47 INFO DAGScheduler: Parents of final stage: List()',\n",
       " '15/10/16 13:15:47 INFO DAGScheduler: Missing parents: List()',\n",
       " '15/10/16 13:15:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at showString at NativeMethodAccessorImpl.java:-2), which has no missing parents',\n",
       " '15/10/16 13:15:47 INFO MemoryStore: ensureFreeSpace(5304) called with curMem=0, maxMem=278302556',\n",
       " '15/10/16 13:15:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.2 KB, free 265.4 MB)',\n",
       " '15/10/16 13:15:47 INFO MemoryStore: ensureFreeSpace(2733) called with curMem=5304, maxMem=278302556',\n",
       " '15/10/16 13:15:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.7 KB, free 265.4 MB)',\n",
       " '15/10/16 13:15:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:44407 (size: 2.7 KB, free: 265.4 MB)',\n",
       " '15/10/16 13:15:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:874',\n",
       " '15/10/16 13:15:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at showString at NativeMethodAccessorImpl.java:-2)',\n",
       " '15/10/16 13:15:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks',\n",
       " '15/10/16 13:15:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1171 bytes)',\n",
       " '15/10/16 13:15:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)',\n",
       " '15/10/16 13:15:47 INFO JDBCRDD: closed connection',\n",
       " '15/10/16 13:15:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 4985 bytes result sent to driver',\n",
       " '15/10/16 13:15:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 366 ms on localhost (1/1)',\n",
       " '15/10/16 13:15:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool ',\n",
       " '15/10/16 13:15:47 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:-2) finished in 0.382 s',\n",
       " '15/10/16 13:15:47 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:-2, took 0.569826 s',\n",
       " '[I 13:16:18.370 NotebookApp] Saving file at /Untitled.ipynb',\n",
       " '[I 13:18:18.364 NotebookApp] Saving file at /Untitled.ipynb',\n",
       " '[I 13:20:18.402 NotebookApp] Saving file at /Untitled.ipynb',\n",
       " '15/10/16 13:31:24 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:-2',\n",
       " '15/10/16 13:31:24 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:-2) with 1 output partitions (allowLocal=false)',\n",
       " '15/10/16 13:31:24 INFO DAGScheduler: Final stage: ResultStage 1(showString at NativeMethodAccessorImpl.java:-2)',\n",
       " '15/10/16 13:31:24 INFO DAGScheduler: Parents of final stage: List()',\n",
       " '15/10/16 13:31:24 INFO DAGScheduler: Missing parents: List()',\n",
       " '15/10/16 13:31:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:-2), which has no missing parents',\n",
       " '15/10/16 13:31:24 INFO MemoryStore: ensureFreeSpace(5304) called with curMem=8037, maxMem=278302556',\n",
       " '15/10/16 13:31:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.2 KB, free 265.4 MB)',\n",
       " '15/10/16 13:31:24 INFO MemoryStore: ensureFreeSpace(2732) called with curMem=13341, maxMem=278302556',\n",
       " '15/10/16 13:31:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.7 KB, free 265.4 MB)',\n",
       " '15/10/16 13:31:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:44407 (size: 2.7 KB, free: 265.4 MB)',\n",
       " '15/10/16 13:31:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874',\n",
       " '15/10/16 13:31:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:-2)',\n",
       " '15/10/16 13:31:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks',\n",
       " '15/10/16 13:31:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1171 bytes)',\n",
       " '15/10/16 13:31:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)',\n",
       " '15/10/16 13:31:24 INFO JDBCRDD: closed connection',\n",
       " '15/10/16 13:31:24 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4995 bytes result sent to driver',\n",
       " '15/10/16 13:31:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 308 ms on localhost (1/1)',\n",
       " '15/10/16 13:31:24 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:-2) finished in 0.310 s',\n",
       " '15/10/16 13:31:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ',\n",
       " '15/10/16 13:31:24 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:-2, took 0.323202 s',\n",
       " '[I 13:32:19.833 NotebookApp] Saving file at /Untitled.ipynb',\n",
       " '[I 13:44:19.751 NotebookApp] Saving file at /Untitled.ipynb',\n",
       " '[I 13:46:30.101 NotebookApp] Saving file at /Untitled.ipynb',\n",
       " '[I 13:50:32.798 NotebookApp] Saving file at /Untitled.ipynb',\n",
       " '[I 13:56:34.101 NotebookApp] Saving file at /Untitled.ipynb',\n",
       " '[W 14:52:15.514 NotebookApp] Notebook Untitled0.ipynb is not trusted',\n",
       " '[I 14:52:15.614 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " '[I 14:52:15.614 NotebookApp] Provisioning local kernel: python2',\n",
       " '[I 14:52:15.643 NotebookApp] Kernel started: d815d372-19b2-4620-80b3-1126c94b76ff',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/16 14:52:18 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/16 14:52:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/16 14:52:19 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/16 14:52:19 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/16 14:52:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/16 14:52:20 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/16 14:52:20 INFO Remoting: Starting remoting',\n",
       " '15/10/16 14:52:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:43750]',\n",
       " \"15/10/16 14:52:20 INFO Utils: Successfully started service 'sparkDriver' on port 43750.\",\n",
       " '15/10/16 14:52:20 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/16 14:52:20 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/16 14:52:20 INFO DiskBlockManager: Created local directory at /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4/blockmgr-73dbe021-6e2b-43f9-9547-72004cf3a221',\n",
       " '15/10/16 14:52:20 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/16 14:52:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4/httpd-a9ac31c5-fdd1-4437-a29f-771847924c71',\n",
       " '15/10/16 14:52:20 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/16 14:52:20 INFO Utils: Successfully started service 'HTTP file server' on port 54419.\",\n",
       " '15/10/16 14:52:20 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/16 14:52:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
       " \"15/10/16 14:52:21 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\",\n",
       " \"15/10/16 14:52:21 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\",\n",
       " \"15/10/16 14:52:21 INFO Utils: Successfully started service 'SparkUI' on port 4043.\",\n",
       " '15/10/16 14:52:21 INFO SparkUI: Started SparkUI at http://172.17.0.22:4043',\n",
       " '15/10/16 14:52:21 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/16 14:52:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54796.\",\n",
       " '15/10/16 14:52:21 INFO NettyBlockTransferService: Server created on 54796',\n",
       " '15/10/16 14:52:21 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/16 14:52:21 INFO BlockManagerMasterEndpoint: Registering block manager localhost:54796 with 265.4 MB RAM, BlockManagerId(driver, localhost, 54796)',\n",
       " '15/10/16 14:52:21 INFO BlockManagerMaster: Registered BlockManager',\n",
       " '[I 14:54:16.025 NotebookApp] Saving file at /Untitled0.ipynb',\n",
       " '[I 14:56:15.996 NotebookApp] Saving file at /Untitled0.ipynb',\n",
       " '[I 14:58:15.961 NotebookApp] Saving file at /Untitled0.ipynb',\n",
       " '[I 15:00:16.003 NotebookApp] Saving file at /RedRock.ipynb',\n",
       " '15/10/16 15:05:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:44407 in memory (size: 2.7 KB, free: 265.4 MB)',\n",
       " '15/10/16 15:05:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:44407 in memory (size: 2.7 KB, free: 265.4 MB)',\n",
       " '[I 15:06:39.589 NotebookApp] Saving file at /Untitled.ipynb',\n",
       " '[W 15:21:41.764 NotebookApp] WebSocket ping timeout after 119954 ms.',\n",
       " '[W 15:21:41.935 NotebookApp] WebSocket ping timeout after 119956 ms.',\n",
       " '[W 15:21:51.950 NotebookApp] WebSocket ping timeout after 119960 ms.',\n",
       " '[I 15:34:33.779 NotebookApp] Saving file at /RedRock.ipynb',\n",
       " '[W 15:42:31.442 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 15:42:31.443 NotebookApp] zmq message arrived on closed channel',\n",
       " '[I 15:56:31.474 NotebookApp] Saving file at /RedRock.ipynb',\n",
       " '[W 16:06:28.475 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:06:28.476 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:06:28.522 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:06:30.872 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:06:30.873 NotebookApp] zmq message arrived on closed channel',\n",
       " '[I 16:06:31.463 NotebookApp] Saving file at /RedRock.ipynb',\n",
       " '[W 16:06:32.970 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:06:32.972 NotebookApp] zmq message arrived on closed channel',\n",
       " '[I 16:08:31.474 NotebookApp] Saving file at /RedRock.ipynb',\n",
       " '[W 16:11:34.069 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:11:34.070 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:11:36.180 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:11:36.181 NotebookApp] zmq message arrived on closed channel',\n",
       " '[I 16:12:33.440 NotebookApp] Saving file at /RedRock.ipynb',\n",
       " '[W 16:14:27.808 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:14:27.809 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:14:28.901 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:14:28.902 NotebookApp] zmq message arrived on closed channel',\n",
       " '[I 16:14:33.752 NotebookApp] Saving file at /RedRock.ipynb',\n",
       " '[W 16:14:37.695 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:14:37.695 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:14:39.776 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 16:14:39.777 NotebookApp] zmq message arrived on closed channel',\n",
       " '[I 16:16:33.776 NotebookApp] Saving file at /RedRock.ipynb',\n",
       " '[W 17:28:01.401 NotebookApp] WebSocket ping timeout after 119960 ms.',\n",
       " '[W 19:48:50.155 NotebookApp] WebSocket ping timeout after 119961 ms.',\n",
       " '[W 21:30:12.489 NotebookApp] WebSocket ping timeout after 119979 ms.',\n",
       " '[W 23:57:57.101 NotebookApp] WebSocket ping timeout after 90001 ms.',\n",
       " '[W 02:42:32.416 NotebookApp] WebSocket ping timeout after 119958 ms.',\n",
       " '[W 05:12:04.531 NotebookApp] WebSocket ping timeout after 119906 ms.',\n",
       " '[W 06:09:14.793 NotebookApp] Notebook RedRock_HM_v3.ipynb is not trusted',\n",
       " '[I 06:09:16.755 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " '[I 06:09:16.755 NotebookApp] Provisioning local kernel: python2',\n",
       " '[I 06:09:16.791 NotebookApp] Kernel started: 00a2534c-99f1-478e-a86d-8dae5c34900f',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/21 06:09:19 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/21 06:09:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/21 06:09:20 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/21 06:09:20 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/21 06:09:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/21 06:09:21 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/21 06:09:21 INFO Remoting: Starting remoting',\n",
       " '15/10/21 06:09:21 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:43928]',\n",
       " \"15/10/21 06:09:21 INFO Utils: Successfully started service 'sparkDriver' on port 43928.\",\n",
       " '15/10/21 06:09:21 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/21 06:09:21 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/21 06:09:21 INFO DiskBlockManager: Created local directory at /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a/blockmgr-ed90f7d7-7049-471a-8560-950825742016',\n",
       " '15/10/21 06:09:21 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/21 06:09:21 INFO HttpFileServer: HTTP File server directory is /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a/httpd-3f3cd3ee-81f2-4ba5-be62-ccdb6d62cf52',\n",
       " '15/10/21 06:09:21 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/21 06:09:21 INFO Utils: Successfully started service 'HTTP file server' on port 39189.\",\n",
       " '15/10/21 06:09:21 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/21 06:09:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
       " \"15/10/21 06:09:22 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\",\n",
       " \"15/10/21 06:09:22 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\",\n",
       " \"15/10/21 06:09:22 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\",\n",
       " \"15/10/21 06:09:22 INFO Utils: Successfully started service 'SparkUI' on port 4044.\",\n",
       " '15/10/21 06:09:22 INFO SparkUI: Started SparkUI at http://172.17.0.22:4044',\n",
       " '15/10/21 06:09:22 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/21 06:09:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34705.\",\n",
       " '15/10/21 06:09:22 INFO NettyBlockTransferService: Server created on 34705',\n",
       " '15/10/21 06:09:22 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/21 06:09:22 INFO BlockManagerMasterEndpoint: Registering block manager localhost:34705 with 265.4 MB RAM, BlockManagerId(driver, localhost, 34705)',\n",
       " '15/10/21 06:09:22 INFO BlockManagerMaster: Registered BlockManager',\n",
       " '--2015-10-21 06:10:05--  https://ibm.box.com/shared/static/pdeifd6iku04877aec39h71qtfgn3c84.csv',\n",
       " 'Resolving ibm.box.com (ibm.box.com)... 74.112.185.182, 74.112.184.85',\n",
       " 'Connecting to ibm.box.com (ibm.box.com)|74.112.185.182|:443... connected.',\n",
       " 'HTTP request sent, awaiting response... 302 Found',\n",
       " 'Location: https://ibm.app.box.com/shared/static/pdeifd6iku04877aec39h71qtfgn3c84.csv [following]',\n",
       " '--2015-10-21 06:10:06--  https://ibm.app.box.com/shared/static/pdeifd6iku04877aec39h71qtfgn3c84.csv',\n",
       " 'Resolving ibm.app.box.com (ibm.app.box.com)... 74.112.184.87, 74.112.185.87',\n",
       " 'Connecting to ibm.app.box.com (ibm.app.box.com)|74.112.184.87|:443... connected.',\n",
       " 'HTTP request sent, awaiting response... 302 Found',\n",
       " 'Location: https://dl.boxcloud.com/d/1/w19ZVDnxNkDhmAbGB1Sp13MB9orJHIuk3Vv6Y_ScJqJ_oVNAbbs_kTETO36WLIG_edmK5um3mQZVm33NnOUIVynE3wSQejZpSJaWrwImdgK7H__lyUw0Hstea68L-UdXnLEmhcUTCnViarvsxxdKChBJ2q6NzYdfRrWCkaDHIo554ZZuKkVGz3RIz8QpZtUy9kZRz0wmry0zk_WiFRMjSAKtBqVBqQoJZsqwrFBW7MWwotNS02lXxOMEqTmap1iz_THDMu6TLNm_xD_K2Z4vbEawweFGDeOA8f_RncME-UVd7NskVcaNAGSXzPvpM2luSirbteRhBWaOERiaWqNCmjS-dqEFO2WEgwiwG0SiwWKXR1z9uHk05LCzSIMR1wHHMQ4L0NI05Oyt4nacqLrXGGPJLE-yrKwIo0QyPYigMIW1igYhZht0T7YKWNZykjn_pYnSb9F808bAbb1i3KyHzKl2u3hrZhMV0WbKIiqZ26musP1rkEQn2Nw44_nXJv9loa_89Jy98Zo-ZDAdn4-mL-2lYEVLOioKLZQ4hS6QD-HB-v74hvOXgrxVu8MEUhrpp_EgqjS5Is02VSZIEwYR-wARacsQKMbGoQNtacJunZkb5I_gqkEuBCrdr6ndnH_qupRaP1cc8ITRZ0szjcd9xBhmYFNsPGaBJeeYYnmeDrvPyc50PJTuM3N3AmS9QsbRF-w-Y3VZilu2O3Tbyu0tYirRDM_qgLzkXbuV40nIxJI6am3TzUdmZuejTsnx2USo7_SNwQb5SkZyPJ-wDWrE8XeaLtvIw6RIOjmz3_hZ5LbS53fOqu-90R7Sz6lAMmt3zbrCfPi-IDmNyl9lyDK_Q7bh4PjK7EcVFmXMj0Ok2ZDFnEdmZz9ApKrIEB4d06ImtKHy7mcc7WQXiLZE71IkLoJdCKDagzJWXL0mpqBD7aG0JjGBumeKEoj_9j8FYpLd042DbSuRYIP6-QGTWGlwWIuZ_0q4r_nssse4hXfoboGJMPkczTSSUQMLHtEAC50ddL-E_xnScfLP5HmCtTWCH0hGwJcwapqdlz-PJ8MAI0VtEixLGRJSgO5He9KYsODVtIo1WUkYGiXdZ9YYySTE4hsPSq9IwWco9zpm2pGUiS0pR6ET1Yw2vUUS-6yw7b6JwLtLnJItaKcudkRl7ibkhpEVkLynIGNQbCrYQbioGXwomyfDNjICg2tg2cL5t7fphw../download [following]',\n",
       " '--2015-10-21 06:10:06--  https://dl.boxcloud.com/d/1/w19ZVDnxNkDhmAbGB1Sp13MB9orJHIuk3Vv6Y_ScJqJ_oVNAbbs_kTETO36WLIG_edmK5um3mQZVm33NnOUIVynE3wSQejZpSJaWrwImdgK7H__lyUw0Hstea68L-UdXnLEmhcUTCnViarvsxxdKChBJ2q6NzYdfRrWCkaDHIo554ZZuKkVGz3RIz8QpZtUy9kZRz0wmry0zk_WiFRMjSAKtBqVBqQoJZsqwrFBW7MWwotNS02lXxOMEqTmap1iz_THDMu6TLNm_xD_K2Z4vbEawweFGDeOA8f_RncME-UVd7NskVcaNAGSXzPvpM2luSirbteRhBWaOERiaWqNCmjS-dqEFO2WEgwiwG0SiwWKXR1z9uHk05LCzSIMR1wHHMQ4L0NI05Oyt4nacqLrXGGPJLE-yrKwIo0QyPYigMIW1igYhZht0T7YKWNZykjn_pYnSb9F808bAbb1i3KyHzKl2u3hrZhMV0WbKIiqZ26musP1rkEQn2Nw44_nXJv9loa_89Jy98Zo-ZDAdn4-mL-2lYEVLOioKLZQ4hS6QD-HB-v74hvOXgrxVu8MEUhrpp_EgqjS5Is02VSZIEwYR-wARacsQKMbGoQNtacJunZkb5I_gqkEuBCrdr6ndnH_qupRaP1cc8ITRZ0szjcd9xBhmYFNsPGaBJeeYYnmeDrvPyc50PJTuM3N3AmS9QsbRF-w-Y3VZilu2O3Tbyu0tYirRDM_qgLzkXbuV40nIxJI6am3TzUdmZuejTsnx2USo7_SNwQb5SkZyPJ-wDWrE8XeaLtvIw6RIOjmz3_hZ5LbS53fOqu-90R7Sz6lAMmt3zbrCfPi-IDmNyl9lyDK_Q7bh4PjK7EcVFmXMj0Ok2ZDFnEdmZz9ApKrIEB4d06ImtKHy7mcc7WQXiLZE71IkLoJdCKDagzJWXL0mpqBD7aG0JjGBumeKEoj_9j8FYpLd042DbSuRYIP6-QGTWGlwWIuZ_0q4r_nssse4hXfoboGJMPkczTSSUQMLHtEAC50ddL-E_xnScfLP5HmCtTWCH0hGwJcwapqdlz-PJ8MAI0VtEixLGRJSgO5He9KYsODVtIo1WUkYGiXdZ9YYySTE4hsPSq9IwWco9zpm2pGUiS0pR6ET1Yw2vUUS-6yw7b6JwLtLnJItaKcudkRl7ibkhpEVkLynIGNQbCrYQbioGXwomyfDNjICg2tg2cL5t7fphw../download',\n",
       " 'Resolving dl.boxcloud.com (dl.boxcloud.com)... 74.112.185.96, 74.112.184.96',\n",
       " 'Connecting to dl.boxcloud.com (dl.boxcloud.com)|74.112.185.96|:443... connected.',\n",
       " 'HTTP request sent, awaiting response... 200 OK',\n",
       " 'Length: 113549 (111K) [text/csv]',\n",
       " 'Saving to: ‘tweetlocation.csv’',\n",
       " '',\n",
       " '',\n",
       " ' 0% [                                       ] 0           --.-K/s              ',\n",
       " '68% [=========================>             ] 77,466       294KB/s             ',\n",
       " '100%[======================================>] 113,549      345KB/s   in 0.3s   ',\n",
       " '',\n",
       " '2015-10-21 06:10:07 (345 KB/s) - ‘tweetlocation.csv’ saved [113549/113549]',\n",
       " '',\n",
       " '--2015-10-21 06:10:08--  https://ibm.box.com/shared/static/wq8g26futqbq0cr2hymg03immnmonw9t.json',\n",
       " 'Resolving ibm.box.com (ibm.box.com)... 74.112.185.182, 74.112.184.85',\n",
       " 'Connecting to ibm.box.com (ibm.box.com)|74.112.185.182|:443... connected.',\n",
       " 'HTTP request sent, awaiting response... 302 Found',\n",
       " 'Location: https://ibm.app.box.com/shared/static/wq8g26futqbq0cr2hymg03immnmonw9t.json [following]',\n",
       " '--2015-10-21 06:10:08--  https://ibm.app.box.com/shared/static/wq8g26futqbq0cr2hymg03immnmonw9t.json',\n",
       " 'Resolving ibm.app.box.com (ibm.app.box.com)... 74.112.185.87, 74.112.184.87',\n",
       " 'Connecting to ibm.app.box.com (ibm.app.box.com)|74.112.185.87|:443... connected.',\n",
       " 'HTTP request sent, awaiting response... 302 Found',\n",
       " 'Location: https://dl.boxcloud.com/d/1/IgYC8Y8pQga4l2_cc2jcaRzsN6HZu8Lhl0kjLJe7rrz2ietinecTAL8JMSgvyn7OgibrcdRzY1rGfQuBwe0nawH5Ypxb_G_F-05vyM2V16eMHpoR3ndojfsvb58Mu49UcyTVBIv7rPPHoIOuP-wFKRrFjSSZpg3DwRq-Za3qlA4VGh9X_EO-4gsUsTIOJ0i5p-ewMPABvpkFFDjUC3AZmNY0ZrQ0GVdRiADIoMzVkEoou8OdouMYEDBWZh3qQsQv-qDX-554Z8gvvi7-fNQkkZ7ufVin3C3uihExlcc5rTXuglyZMXTuMBrwurWpch19vqOWkrfl25-UFZYeAreDJGgyN24kXwPhUfiq281lU12TZcCiOJBBEgVXj86lhztkOlzZLJi04WFDXBetyIxwsdotzF_TcXwa-gscmSZZOettdQXuEuZc3zVbvC3dlA5DM_IbVYsc00F62hLsJ8toY-f_T1LTKAX-4lVDuGliakqu9oGl8DLA0y7NT2TGP2G-OyNnmW6l_iQ_giXH-GwJW1cp_8ZxJ8lVErUCONRM6CDcNe3mS8xwh7KFtIocgMRtudHQMK6qCPe9Z2_SMYOqOG1nP7vaN53L5CYz6xRL9RoCEWx_BMnbZ5Qe2xV7Ct1p1Zb6IgbykLVD3lXUUD5-oFKMLTXZ9l2-BslVkm-D_kgqmEb1yB7C3MAh3uDp9zYrTw0w9N-E4xv1C5VKuTq2f4874VkCWqFfrDK12KIRoyHaFVeafkgyvodAog91F-GsUynHAkXZI7TgJqzugKXT5ccGQbsvv77kMJ7juHEIH7vysL6yK2_x4tCoi98kADSFUUOQ53twF2UZ7C8E1LSiqYWBtUhdWFF5f68GRftZav4v2uduDCFpSnDZH-fwv1tLMMwkw2jMetodUoOhJX7e19y7ydv_9uNrSSelhmILq-3T_r8g27etAuD4hYByQCxfXQsNWa9UlF5HohNscEGky3Fgf6pGwquHLQUZnM9UTSpbOO-Hhm2gPwEQntTifWNriUbgzMH5JnggIwZgMQdzp5e3r_GP-NYKKaMpfUWqXY76wxk8onZxix5P7I4KQiGNaSi1NBChKjh990ovHjAnAPNDBjZ09SjMmOGnIEh6MASv-V6EGTDGNQz0ynV1CVnaEIOW9DKcTpl-ND6n8YQkn4upwjaWcuM6G3YAwQ6oqUAYWl05BzBTb3lRkMU7hQhu/download [following]',\n",
       " '--2015-10-21 06:10:09--  https://dl.boxcloud.com/d/1/IgYC8Y8pQga4l2_cc2jcaRzsN6HZu8Lhl0kjLJe7rrz2ietinecTAL8JMSgvyn7OgibrcdRzY1rGfQuBwe0nawH5Ypxb_G_F-05vyM2V16eMHpoR3ndojfsvb58Mu49UcyTVBIv7rPPHoIOuP-wFKRrFjSSZpg3DwRq-Za3qlA4VGh9X_EO-4gsUsTIOJ0i5p-ewMPABvpkFFDjUC3AZmNY0ZrQ0GVdRiADIoMzVkEoou8OdouMYEDBWZh3qQsQv-qDX-554Z8gvvi7-fNQkkZ7ufVin3C3uihExlcc5rTXuglyZMXTuMBrwurWpch19vqOWkrfl25-UFZYeAreDJGgyN24kXwPhUfiq281lU12TZcCiOJBBEgVXj86lhztkOlzZLJi04WFDXBetyIxwsdotzF_TcXwa-gscmSZZOettdQXuEuZc3zVbvC3dlA5DM_IbVYsc00F62hLsJ8toY-f_T1LTKAX-4lVDuGliakqu9oGl8DLA0y7NT2TGP2G-OyNnmW6l_iQ_giXH-GwJW1cp_8ZxJ8lVErUCONRM6CDcNe3mS8xwh7KFtIocgMRtudHQMK6qCPe9Z2_SMYOqOG1nP7vaN53L5CYz6xRL9RoCEWx_BMnbZ5Qe2xV7Ct1p1Zb6IgbykLVD3lXUUD5-oFKMLTXZ9l2-BslVkm-D_kgqmEb1yB7C3MAh3uDp9zYrTw0w9N-E4xv1C5VKuTq2f4874VkCWqFfrDK12KIRoyHaFVeafkgyvodAog91F-GsUynHAkXZI7TgJqzugKXT5ccGQbsvv77kMJ7juHEIH7vysL6yK2_x4tCoi98kADSFUUOQ53twF2UZ7C8E1LSiqYWBtUhdWFF5f68GRftZav4v2uduDCFpSnDZH-fwv1tLMMwkw2jMetodUoOhJX7e19y7ydv_9uNrSSelhmILq-3T_r8g27etAuD4hYByQCxfXQsNWa9UlF5HohNscEGky3Fgf6pGwquHLQUZnM9UTSpbOO-Hhm2gPwEQntTifWNriUbgzMH5JnggIwZgMQdzp5e3r_GP-NYKKaMpfUWqXY76wxk8onZxix5P7I4KQiGNaSi1NBChKjh990ovHjAnAPNDBjZ09SjMmOGnIEh6MASv-V6EGTDGNQz0ynV1CVnaEIOW9DKcTpl-ND6n8YQkn4upwjaWcuM6G3YAwQ6oqUAYWl05BzBTb3lRkMU7hQhu/download',\n",
       " 'Resolving dl.boxcloud.com (dl.boxcloud.com)... 74.112.185.96, 74.112.184.96',\n",
       " 'Connecting to dl.boxcloud.com (dl.boxcloud.com)|74.112.185.96|:443... connected.',\n",
       " 'HTTP request sent, awaiting response... 200 OK',\n",
       " 'Length: 1895828 (1.8M) [application/octet-stream]',\n",
       " 'Saving to: ‘zip3.json’',\n",
       " '',\n",
       " '',\n",
       " ' 0% [                                       ] 0           --.-K/s              ',\n",
       " ' 4% [>                                      ] 77,465       287KB/s             ',\n",
       " '11% [===>                                   ] 224,921      441KB/s             ',\n",
       " '23% [========>                              ] 454,297      613KB/s             ',\n",
       " '47% [=================>                     ] 896,665      947KB/s             ',\n",
       " '84% [===============================>       ] 1,601,177   1.32MB/s             ',\n",
       " '100%[======================================>] 1,895,828   1.48MB/s   in 1.2s   ',\n",
       " '',\n",
       " '2015-10-21 06:10:11 (1.48 MB/s) - ‘zip3.json’ saved [1895828/1895828]',\n",
       " '',\n",
       " '[I 06:11:25.852 NotebookApp] Saving file at /RedRock_HM_v3.ipynb',\n",
       " 'Map: Tweet locations',\n",
       " '[W 06:11:33.185 NotebookApp] Deprecated files/ URL: files/map.html',\n",
       " '[I 06:11:33.186 NotebookApp] 302 GET /notebooks/files/map.html (172.17.0.29) 1.75ms',\n",
       " '[I 06:13:33.654 NotebookApp] Saving file at /RedRock_HM_v3.ipynb',\n",
       " '[I 06:17:33.675 NotebookApp] Saving file at /RedRock_HM_v3.ipynb',\n",
       " 'Map: Tweet locations',\n",
       " '[W 06:17:42.511 NotebookApp] Deprecated files/ URL: files/map.html',\n",
       " '[I 06:17:42.512 NotebookApp] 302 GET /notebooks/files/map.html (172.17.0.29) 1.47ms',\n",
       " '[I 06:18:12.108 NotebookApp] Saving file at /RedRock_HM_v3.ipynb',\n",
       " '[W 06:18:15.690 NotebookApp] Notebook Untitled0.ipynb is not trusted',\n",
       " '[I 06:18:15.770 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " '[I 06:18:15.770 NotebookApp] Provisioning local kernel: python2',\n",
       " '[I 06:18:15.818 NotebookApp] Kernel started: 67458258-930e-4921-94f9-f58353462d87',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/21 06:18:18 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/21 06:18:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/21 06:18:19 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/21 06:18:19 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/21 06:18:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/21 06:18:20 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/21 06:18:20 INFO Remoting: Starting remoting',\n",
       " '15/10/21 06:18:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:34767]',\n",
       " \"15/10/21 06:18:20 INFO Utils: Successfully started service 'sparkDriver' on port 34767.\",\n",
       " '15/10/21 06:18:20 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/21 06:18:20 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/21 06:18:20 INFO DiskBlockManager: Created local directory at /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350/blockmgr-5abb109b-5c36-4d13-ac93-7ad13e807555',\n",
       " '15/10/21 06:18:20 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/21 06:18:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350/httpd-81687cf4-f5a6-4a97-8e52-a6096ad60235',\n",
       " '15/10/21 06:18:20 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/21 06:18:20 INFO Utils: Successfully started service 'HTTP file server' on port 47914.\",\n",
       " '15/10/21 06:18:20 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/21 06:18:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
       " \"15/10/21 06:18:21 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\",\n",
       " \"15/10/21 06:18:21 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\",\n",
       " \"15/10/21 06:18:21 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\",\n",
       " \"15/10/21 06:18:21 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\",\n",
       " \"15/10/21 06:18:21 INFO Utils: Successfully started service 'SparkUI' on port 4045.\",\n",
       " '15/10/21 06:18:21 INFO SparkUI: Started SparkUI at http://172.17.0.22:4045',\n",
       " '15/10/21 06:18:21 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/21 06:18:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58989.\",\n",
       " '15/10/21 06:18:21 INFO NettyBlockTransferService: Server created on 58989',\n",
       " '15/10/21 06:18:21 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/21 06:18:21 INFO BlockManagerMasterEndpoint: Registering block manager localhost:58989 with 265.4 MB RAM, BlockManagerId(driver, localhost, 58989)',\n",
       " '15/10/21 06:18:21 INFO BlockManagerMaster: Registered BlockManager',\n",
       " 'Map: Tweet locations',\n",
       " '<IPython.core.display.HTML object>',\n",
       " 'Map: Tweet locations',\n",
       " '[W 06:19:35.458 NotebookApp] Deprecated files/ URL: files/map.html',\n",
       " '[I 06:19:35.459 NotebookApp] 302 GET /notebooks/files/map.html (172.17.0.29) 1.81ms',\n",
       " '[I 06:20:15.912 NotebookApp] Saving file at /RedRock Update.ipynb',\n",
       " '[I 06:22:16.115 NotebookApp] Saving file at /RedRock Update.ipynb',\n",
       " 'Map: Tweet locations',\n",
       " '[W 06:26:36.433 NotebookApp] Deprecated files/ URL: files/map.html',\n",
       " '[I 06:26:36.434 NotebookApp] 302 GET /notebooks/files/map.html (172.17.0.29) 1.53ms',\n",
       " '[I 06:28:15.938 NotebookApp] Saving file at /RedRock Update.ipynb',\n",
       " '[W 06:44:06.278 NotebookApp] Notebook presentation.ipynb is not trusted',\n",
       " '[W 06:44:06.940 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.940 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.941 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.941 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.941 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.942 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.942 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.943 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.943 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.943 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.943 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.944 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.944 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.944 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.945 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.945 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.946 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:06.946 NotebookApp] zmq message arrived on closed channel',\n",
       " '[I 06:44:15.922 NotebookApp] Saving file at /RedRock Update.ipynb',\n",
       " '[I 06:44:27.078 NotebookApp] Request shutdown_kernel: 12cbbe53-badb-4eee-88e0-08f04221d2be, <IPython.kernel.ioloop.manager.IOLoopKernelManager object at 0x7fd632bb8c10>',\n",
       " '15/10/21 06:44:27 INFO SparkUI: Stopped Spark web UI at http://172.17.0.22:4041',\n",
       " '15/10/21 06:44:27 INFO DAGScheduler: Stopping DAGScheduler',\n",
       " '15/10/21 06:44:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!',\n",
       " '15/10/21 06:44:27 INFO Utils: path = /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46/blockmgr-aad4e583-6a6c-479a-b021-a7e0390ea261, already present as root for deletion.',\n",
       " '15/10/21 06:44:27 INFO MemoryStore: MemoryStore cleared',\n",
       " '15/10/21 06:44:27 INFO BlockManager: BlockManager stopped',\n",
       " '15/10/21 06:44:27 INFO BlockManagerMaster: BlockManagerMaster stopped',\n",
       " '15/10/21 06:44:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!',\n",
       " '15/10/21 06:44:27 INFO SparkContext: Successfully stopped SparkContext',\n",
       " '15/10/21 06:44:27 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.',\n",
       " '15/10/21 06:44:27 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.',\n",
       " '15/10/21 06:44:27 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.',\n",
       " '15/10/21 06:44:27 INFO Utils: Shutdown hook called',\n",
       " '15/10/21 06:44:27 INFO Utils: Deleting directory /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46',\n",
       " '[I 06:44:27.681 NotebookApp] Kernel shutdown: 12cbbe53-badb-4eee-88e0-08f04221d2be',\n",
       " '[W 06:44:27.685 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.685 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.685 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.685 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.686 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.686 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.686 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.686 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.686 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.687 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.687 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.687 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.687 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.688 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.688 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.688 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.688 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.688 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.689 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.689 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.689 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.689 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.689 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.690 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.690 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.690 NotebookApp] zmq message arrived on closed channel',\n",
       " '[W 06:44:27.690 NotebookApp] zmq message arrived on closed channel',\n",
       " '[I 06:44:41.422 NotebookApp] Request shutdown_kernel: 00a2534c-99f1-478e-a86d-8dae5c34900f, <IPython.kernel.ioloop.manager.IOLoopKernelManager object at 0x7fd631831b10>',\n",
       " '15/10/21 06:44:41 INFO SparkUI: Stopped Spark web UI at http://172.17.0.22:4044',\n",
       " '15/10/21 06:44:41 INFO DAGScheduler: Stopping DAGScheduler',\n",
       " '15/10/21 06:44:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!',\n",
       " '15/10/21 06:44:41 INFO Utils: path = /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a/blockmgr-ed90f7d7-7049-471a-8560-950825742016, already present as root for deletion.',\n",
       " '15/10/21 06:44:41 INFO MemoryStore: MemoryStore cleared',\n",
       " '15/10/21 06:44:41 INFO BlockManager: BlockManager stopped',\n",
       " '15/10/21 06:44:41 INFO BlockManagerMaster: BlockManagerMaster stopped',\n",
       " '15/10/21 06:44:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!',\n",
       " '15/10/21 06:44:41 INFO SparkContext: Successfully stopped SparkContext',\n",
       " '15/10/21 06:44:41 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.',\n",
       " '15/10/21 06:44:41 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.',\n",
       " '15/10/21 06:44:41 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.',\n",
       " '[I 06:44:42.442 NotebookApp] Kernel shutdown: 00a2534c-99f1-478e-a86d-8dae5c34900f',\n",
       " '15/10/21 06:44:42 INFO Utils: Shutdown hook called',\n",
       " '15/10/21 06:44:42 INFO Utils: Deleting directory /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a',\n",
       " '[I 06:45:58.919 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " '[I 06:45:58.920 NotebookApp] Provisioning local kernel: python2',\n",
       " '[I 06:45:58.969 NotebookApp] Kernel started: 18a89cff-0214-4ef7-94d1-a3a69acdfe11',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/21 06:46:01 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/21 06:46:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/21 06:46:02 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/21 06:46:02 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/21 06:46:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/21 06:46:03 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/21 06:46:03 INFO Remoting: Starting remoting',\n",
       " '15/10/21 06:46:03 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:44681]',\n",
       " \"15/10/21 06:46:03 INFO Utils: Successfully started service 'sparkDriver' on port 44681.\",\n",
       " '15/10/21 06:46:03 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/21 06:46:03 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/21 06:46:03 INFO DiskBlockManager: Created local directory at /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5/blockmgr-719b62d4-5020-486a-bf06-ff030c696f62',\n",
       " '15/10/21 06:46:03 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/21 06:46:04 INFO HttpFileServer: HTTP File server directory is /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5/httpd-ca2b9527-9689-44df-90b9-94eb76bf22c8',\n",
       " '15/10/21 06:46:04 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/21 06:46:04 INFO Utils: Successfully started service 'HTTP file server' on port 58880.\",\n",
       " '15/10/21 06:46:04 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/21 06:46:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
       " \"15/10/21 06:46:04 INFO Utils: Successfully started service 'SparkUI' on port 4041.\",\n",
       " '15/10/21 06:46:04 INFO SparkUI: Started SparkUI at http://172.17.0.22:4041',\n",
       " '15/10/21 06:46:04 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/21 06:46:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33807.\",\n",
       " '15/10/21 06:46:04 INFO NettyBlockTransferService: Server created on 33807',\n",
       " '15/10/21 06:46:04 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/21 06:46:04 INFO BlockManagerMasterEndpoint: Registering block manager localhost:33807 with 265.4 MB RAM, BlockManagerId(driver, localhost, 33807)',\n",
       " '15/10/21 06:46:04 INFO BlockManagerMaster: Registered BlockManager',\n",
       " '[I 06:46:05.846 NotebookApp] Request shutdown_kernel: 18a89cff-0214-4ef7-94d1-a3a69acdfe11, <IPython.kernel.ioloop.manager.IOLoopKernelManager object at 0x7fd63184d610>',\n",
       " '15/10/21 06:46:06 INFO SparkUI: Stopped Spark web UI at http://172.17.0.22:4041',\n",
       " '15/10/21 06:46:06 INFO DAGScheduler: Stopping DAGScheduler',\n",
       " '15/10/21 06:46:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!',\n",
       " '15/10/21 06:46:06 INFO Utils: path = /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5/blockmgr-719b62d4-5020-486a-bf06-ff030c696f62, already present as root for deletion.',\n",
       " '15/10/21 06:46:06 INFO MemoryStore: MemoryStore cleared',\n",
       " '15/10/21 06:46:06 INFO BlockManager: BlockManager stopped',\n",
       " '15/10/21 06:46:06 INFO BlockManagerMaster: BlockManagerMaster stopped',\n",
       " '15/10/21 06:46:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!',\n",
       " '15/10/21 06:46:06 INFO SparkContext: Successfully stopped SparkContext',\n",
       " '15/10/21 06:46:06 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.',\n",
       " '15/10/21 06:46:06 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.',\n",
       " '15/10/21 06:46:06 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.',\n",
       " '15/10/21 06:46:06 INFO Utils: Shutdown hook called',\n",
       " '15/10/21 06:46:06 INFO Utils: Deleting directory /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5',\n",
       " '[I 06:46:06.649 NotebookApp] Kernel shutdown: 18a89cff-0214-4ef7-94d1-a3a69acdfe11',\n",
       " '[I 06:46:15.464 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " '[I 06:46:15.464 NotebookApp] Provisioning local kernel: python2',\n",
       " '[I 06:46:15.510 NotebookApp] Kernel started: c7f66d04-76bc-4f9a-a9f8-a47936dc6ef0',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " '[I 06:46:18.059 NotebookApp] Request shutdown_kernel: 67458258-930e-4921-94f9-f58353462d87, <IPython.kernel.ioloop.manager.IOLoopKernelManager object at 0x7fd63184d690>',\n",
       " '15/10/21 06:46:18 INFO SparkUI: Stopped Spark web UI at http://172.17.0.22:4045',\n",
       " '15/10/21 06:46:18 INFO DAGScheduler: Stopping DAGScheduler',\n",
       " '15/10/21 06:46:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!',\n",
       " '15/10/21 06:46:18 INFO Utils: path = /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350/blockmgr-5abb109b-5c36-4d13-ac93-7ad13e807555, already present as root for deletion.',\n",
       " '15/10/21 06:46:18 INFO MemoryStore: MemoryStore cleared',\n",
       " '15/10/21 06:46:18 INFO BlockManager: BlockManager stopped',\n",
       " '15/10/21 06:46:18 INFO BlockManagerMaster: BlockManagerMaster stopped',\n",
       " '15/10/21 06:46:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!',\n",
       " '15/10/21 06:46:18 INFO SparkContext: Successfully stopped SparkContext',\n",
       " '15/10/21 06:46:18 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.',\n",
       " '15/10/21 06:46:18 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.',\n",
       " '15/10/21 06:46:18 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/21 06:46:18 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/21 06:46:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '[I 06:46:19.078 NotebookApp] Kernel shutdown: 67458258-930e-4921-94f9-f58353462d87',\n",
       " '15/10/21 06:46:19 INFO Utils: Shutdown hook called',\n",
       " '15/10/21 06:46:19 INFO Utils: Deleting directory /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350',\n",
       " '15/10/21 06:46:19 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/21 06:46:19 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/21 06:46:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/21 06:46:20 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/21 06:46:20 INFO Remoting: Starting remoting',\n",
       " '15/10/21 06:46:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:34749]',\n",
       " \"15/10/21 06:46:20 INFO Utils: Successfully started service 'sparkDriver' on port 34749.\",\n",
       " '15/10/21 06:46:20 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/21 06:46:20 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/21 06:46:20 INFO DiskBlockManager: Created local directory at /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/blockmgr-fb8c79d9-cb49-4e14-9eae-02211819594f',\n",
       " '15/10/21 06:46:20 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/21 06:46:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/httpd-72d3e35d-a6b4-427b-b7cd-7f40b45041ae',\n",
       " '15/10/21 06:46:20 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/21 06:46:20 INFO Utils: Successfully started service 'HTTP file server' on port 56995.\",\n",
       " '15/10/21 06:46:20 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/21 06:46:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
       " \"15/10/21 06:46:20 INFO Utils: Successfully started service 'SparkUI' on port 4041.\",\n",
       " '15/10/21 06:46:20 INFO SparkUI: Started SparkUI at http://172.17.0.22:4041',\n",
       " '15/10/21 06:46:21 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/21 06:46:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43363.\",\n",
       " '15/10/21 06:46:21 INFO NettyBlockTransferService: Server created on 43363',\n",
       " '15/10/21 06:46:21 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/21 06:46:21 INFO BlockManagerMasterEndpoint: Registering block manager localhost:43363 with 265.4 MB RAM, BlockManagerId(driver, localhost, 43363)',\n",
       " '15/10/21 06:46:21 INFO BlockManagerMaster: Registered BlockManager',\n",
       " '15/10/21 06:48:09 INFO SparkContext: Starting job: reduce at <ipython-input-1-be560de4c7e1>:14',\n",
       " '15/10/21 06:48:09 INFO DAGScheduler: Got job 0 (reduce at <ipython-input-1-be560de4c7e1>:14) with 2 output partitions (allowLocal=false)',\n",
       " '15/10/21 06:48:09 INFO DAGScheduler: Final stage: ResultStage 0(reduce at <ipython-input-1-be560de4c7e1>:14)',\n",
       " '15/10/21 06:48:09 INFO DAGScheduler: Parents of final stage: List()',\n",
       " '15/10/21 06:48:09 INFO DAGScheduler: Missing parents: List()',\n",
       " '15/10/21 06:48:09 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at reduce at <ipython-input-1-be560de4c7e1>:14), which has no missing parents',\n",
       " '15/10/21 06:48:09 INFO MemoryStore: ensureFreeSpace(4392) called with curMem=0, maxMem=278302556',\n",
       " '15/10/21 06:48:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.3 KB, free 265.4 MB)',\n",
       " '15/10/21 06:48:09 INFO MemoryStore: ensureFreeSpace(2771) called with curMem=4392, maxMem=278302556',\n",
       " '15/10/21 06:48:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.7 KB, free 265.4 MB)',\n",
       " '15/10/21 06:48:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:43363 (size: 2.7 KB, free: 265.4 MB)',\n",
       " '15/10/21 06:48:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:874',\n",
       " '15/10/21 06:48:09 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at reduce at <ipython-input-1-be560de4c7e1>:14)',\n",
       " '15/10/21 06:48:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks',\n",
       " '15/10/21 06:48:09 WARN TaskSetManager: Stage 0 contains a task of very large size (364 KB). The maximum recommended task size is 100 KB.',\n",
       " '15/10/21 06:48:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 373750 bytes)',\n",
       " '15/10/21 06:48:09 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 501553 bytes)',\n",
       " '15/10/21 06:48:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)',\n",
       " '15/10/21 06:48:09 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)',\n",
       " '15/10/21 06:48:13 INFO PythonRDD: Times: total = 3709, boot = 3612, init = 10, finish = 87',\n",
       " '15/10/21 06:48:13 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 701 bytes result sent to driver',\n",
       " '15/10/21 06:48:13 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3771 ms on localhost (1/2)',\n",
       " '15/10/21 06:48:13 INFO PythonRDD: Times: total = 3757, boot = 3666, init = 3, finish = 88',\n",
       " '15/10/21 06:48:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 701 bytes result sent to driver',\n",
       " '15/10/21 06:48:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3838 ms on localhost (2/2)',\n",
       " '15/10/21 06:48:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool ',\n",
       " '15/10/21 06:48:13 INFO DAGScheduler: ResultStage 0 (reduce at <ipython-input-1-be560de4c7e1>:14) finished in 3.853 s',\n",
       " '15/10/21 06:48:13 INFO DAGScheduler: Job 0 finished: reduce at <ipython-input-1-be560de4c7e1>:14, took 4.073024 s',\n",
       " '[I 06:48:15.554 NotebookApp] Saving file at /Spark Code.ipynb',\n",
       " '[I 06:49:54.058 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " '[I 06:49:54.058 NotebookApp] Provisioning local kernel: python2',\n",
       " '[I 06:49:54.103 NotebookApp] Kernel started: d4389eab-b783-4abf-8ae5-ce3bebb2fbd5',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/21 06:49:57 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '[I 06:49:57.147 NotebookApp] Request shutdown_kernel: d4389eab-b783-4abf-8ae5-ce3bebb2fbd5, <IPython.kernel.ioloop.manager.IOLoopKernelManager object at 0x7fd63170c150>',\n",
       " '15/10/21 06:49:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/21 06:49:57 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/21 06:49:57 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/21 06:49:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '[I 06:49:58.159 NotebookApp] Kernel shutdown: d4389eab-b783-4abf-8ae5-ce3bebb2fbd5',\n",
       " '15/10/21 06:49:58 INFO Utils: Shutdown hook called',\n",
       " '[W 06:50:04.173 NotebookApp] Timeout waiting for kernel_info reply from d4389eab-b783-4abf-8ae5-ce3bebb2fbd5',\n",
       " '[E 06:50:04.174 NotebookApp] Error opening stream: HTTP 404: Not Found (Kernel does not exist: d4389eab-b783-4abf-8ae5-ce3bebb2fbd5)',\n",
       " '[I 06:50:19.858 NotebookApp] Saving file at /Spark Code.ipynb',\n",
       " '15/10/21 06:51:28 INFO SparkContext: Starting job: reduce at <ipython-input-2-be560de4c7e1>:14',\n",
       " '15/10/21 06:51:28 INFO DAGScheduler: Got job 1 (reduce at <ipython-input-2-be560de4c7e1>:14) with 2 output partitions (allowLocal=false)',\n",
       " '15/10/21 06:51:28 INFO DAGScheduler: Final stage: ResultStage 1(reduce at <ipython-input-2-be560de4c7e1>:14)',\n",
       " '15/10/21 06:51:28 INFO DAGScheduler: Parents of final stage: List()',\n",
       " '15/10/21 06:51:28 INFO DAGScheduler: Missing parents: List()',\n",
       " '15/10/21 06:51:28 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[3] at reduce at <ipython-input-2-be560de4c7e1>:14), which has no missing parents',\n",
       " '15/10/21 06:51:28 INFO MemoryStore: ensureFreeSpace(4392) called with curMem=7163, maxMem=278302556',\n",
       " '15/10/21 06:51:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 265.4 MB)',\n",
       " '15/10/21 06:51:28 INFO MemoryStore: ensureFreeSpace(2774) called with curMem=11555, maxMem=278302556',\n",
       " '15/10/21 06:51:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.7 KB, free 265.4 MB)',\n",
       " '15/10/21 06:51:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:43363 (size: 2.7 KB, free: 265.4 MB)',\n",
       " '15/10/21 06:51:28 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874',\n",
       " '15/10/21 06:51:28 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[3] at reduce at <ipython-input-2-be560de4c7e1>:14)',\n",
       " '15/10/21 06:51:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks',\n",
       " '15/10/21 06:51:28 WARN TaskSetManager: Stage 1 contains a task of very large size (364 KB). The maximum recommended task size is 100 KB.',\n",
       " '15/10/21 06:51:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, PROCESS_LOCAL, 373750 bytes)',\n",
       " '15/10/21 06:51:28 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, PROCESS_LOCAL, 501553 bytes)',\n",
       " '15/10/21 06:51:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)',\n",
       " '15/10/21 06:51:28 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)',\n",
       " '15/10/21 06:51:28 INFO PythonRDD: Times: total = 149, boot = 59, init = 3, finish = 87',\n",
       " '15/10/21 06:51:28 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 701 bytes result sent to driver',\n",
       " '15/10/21 06:51:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 170 ms on localhost (1/2)',\n",
       " '15/10/21 06:51:28 INFO PythonRDD: Times: total = 202, boot = 113, init = 2, finish = 87',\n",
       " '15/10/21 06:51:28 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 701 bytes result sent to driver',\n",
       " '15/10/21 06:51:28 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 266 ms on localhost (2/2)',\n",
       " '15/10/21 06:51:28 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ',\n",
       " '15/10/21 06:51:28 INFO DAGScheduler: ResultStage 1 (reduce at <ipython-input-2-be560de4c7e1>:14) finished in 0.272 s',\n",
       " '15/10/21 06:51:28 INFO DAGScheduler: Job 1 finished: reduce at <ipython-input-2-be560de4c7e1>:14, took 0.288274 s',\n",
       " '15/10/21 06:51:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:43363 in memory (size: 2.7 KB, free: 265.4 MB)',\n",
       " '[I 06:51:39.348 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " '[I 06:51:39.349 NotebookApp] Provisioning local kernel: python2',\n",
       " '[I 06:51:39.394 NotebookApp] Kernel started: c3da83e4-da41-46b6-931e-b02641be0d98',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/21 06:51:42 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/21 06:51:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/21 06:51:43 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/21 06:51:43 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/21 06:51:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/21 06:51:43 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/21 06:51:43 INFO Remoting: Starting remoting',\n",
       " '15/10/21 06:51:44 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:58291]',\n",
       " \"15/10/21 06:51:44 INFO Utils: Successfully started service 'sparkDriver' on port 58291.\",\n",
       " '15/10/21 06:51:44 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/21 06:51:44 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/21 06:51:44 INFO DiskBlockManager: Created local directory at /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5/blockmgr-22d08b29-3ede-43e5-b659-7938c320c115',\n",
       " '15/10/21 06:51:44 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/21 06:51:44 INFO HttpFileServer: HTTP File server directory is /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5/httpd-c3af5d8f-93b1-4ea1-b4cd-d939821a87ee',\n",
       " '15/10/21 06:51:44 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/21 06:51:44 INFO Utils: Successfully started service 'HTTP file server' on port 33877.\",\n",
       " '15/10/21 06:51:44 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/21 06:51:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
       " \"15/10/21 06:51:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\",\n",
       " \"15/10/21 06:51:44 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\",\n",
       " \"15/10/21 06:51:44 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\",\n",
       " \"15/10/21 06:51:44 INFO Utils: Successfully started service 'SparkUI' on port 4044.\",\n",
       " '15/10/21 06:51:44 INFO SparkUI: Started SparkUI at http://172.17.0.22:4044',\n",
       " '15/10/21 06:51:44 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/21 06:51:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60134.\",\n",
       " '15/10/21 06:51:45 INFO NettyBlockTransferService: Server created on 60134',\n",
       " '15/10/21 06:51:45 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/21 06:51:45 INFO BlockManagerMasterEndpoint: Registering block manager localhost:60134 with 265.4 MB RAM, BlockManagerId(driver, localhost, 60134)',\n",
       " '15/10/21 06:51:45 INFO BlockManagerMaster: Registered BlockManager',\n",
       " 'Map: Tweet locations',\n",
       " '[W 06:51:54.197 NotebookApp] Deprecated files/ URL: files/map.html',\n",
       " '[I 06:51:54.197 NotebookApp] 302 GET /notebooks/files/map.html (172.17.0.29) 1.69ms',\n",
       " '[I 06:52:34.904 NotebookApp] Saving file at /Spark Code.ipynb',\n",
       " '[I 06:53:10.736 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " '[I 06:53:10.737 NotebookApp] Provisioning local kernel: python2',\n",
       " '[I 06:53:10.782 NotebookApp] Kernel started: 30fd377f-e20d-4e26-8333-7a1c861e3e3f',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/21 06:53:13 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/21 06:53:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/21 06:53:14 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/21 06:53:14 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/21 06:53:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/21 06:53:15 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/21 06:53:15 INFO Remoting: Starting remoting',\n",
       " '15/10/21 06:53:15 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:52949]',\n",
       " \"15/10/21 06:53:15 INFO Utils: Successfully started service 'sparkDriver' on port 52949.\",\n",
       " '15/10/21 06:53:15 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/21 06:53:15 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/21 06:53:15 INFO DiskBlockManager: Created local directory at /tmp/spark-a0d30f27-58f2-4803-b7ce-2f437dce18c1/blockmgr-33399bc4-6281-42c6-b023-b7bcd4a56bc2',\n",
       " '15/10/21 06:53:15 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/21 06:53:15 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a0d30f27-58f2-4803-b7ce-2f437dce18c1/httpd-0637bf42-85e3-45a9-a395-9fadcb6744a4',\n",
       " '15/10/21 06:53:15 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/21 06:53:15 INFO Utils: Successfully started service 'HTTP file server' on port 43389.\",\n",
       " '15/10/21 06:53:15 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/21 06:53:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
       " \"15/10/21 06:53:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\",\n",
       " \"15/10/21 06:53:16 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\",\n",
       " \"15/10/21 06:53:16 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\",\n",
       " \"15/10/21 06:53:16 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\",\n",
       " \"15/10/21 06:53:16 INFO Utils: Successfully started service 'SparkUI' on port 4045.\",\n",
       " '15/10/21 06:53:16 INFO SparkUI: Started SparkUI at http://172.17.0.22:4045',\n",
       " '15/10/21 06:53:16 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/21 06:53:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42148.\",\n",
       " '15/10/21 06:53:16 INFO NettyBlockTransferService: Server created on 42148',\n",
       " '15/10/21 06:53:16 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/21 06:53:16 INFO BlockManagerMasterEndpoint: Registering block manager localhost:42148 with 265.4 MB RAM, BlockManagerId(driver, localhost, 42148)',\n",
       " '15/10/21 06:53:16 INFO BlockManagerMaster: Registered BlockManager',\n",
       " '[I 06:53:32.915 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " '[I 06:53:32.916 NotebookApp] Provisioning local kernel: python2',\n",
       " '[I 06:53:32.961 NotebookApp] Kernel started: dd7acff1-b1b3-4bdc-821c-9bb42891498c',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/21 06:53:35 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/21 06:53:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/21 06:53:36 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/21 06:53:36 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/21 06:53:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/21 06:53:37 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/21 06:53:37 INFO Remoting: Starting remoting',\n",
       " '15/10/21 06:53:37 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:48625]',\n",
       " \"15/10/21 06:53:37 INFO Utils: Successfully started service 'sparkDriver' on port 48625.\",\n",
       " '15/10/21 06:53:37 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/21 06:53:37 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/21 06:53:37 INFO DiskBlockManager: Created local directory at /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3/blockmgr-822dc396-71cd-4fe2-893d-9f536687422a',\n",
       " '15/10/21 06:53:37 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/21 06:53:37 INFO HttpFileServer: HTTP File server directory is /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3/httpd-034f2b34-e002-40b1-9500-9409076170ec',\n",
       " '15/10/21 06:53:37 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/21 06:53:38 INFO Utils: Successfully started service 'HTTP file server' on port 55101.\",\n",
       " '15/10/21 06:53:38 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/21 06:53:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
       " \"15/10/21 06:53:38 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\",\n",
       " \"15/10/21 06:53:38 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\",\n",
       " \"15/10/21 06:53:38 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\",\n",
       " \"15/10/21 06:53:38 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\",\n",
       " \"15/10/21 06:53:38 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\",\n",
       " \"15/10/21 06:53:38 INFO Utils: Successfully started service 'SparkUI' on port 4046.\",\n",
       " '15/10/21 06:53:38 INFO SparkUI: Started SparkUI at http://172.17.0.22:4046',\n",
       " '15/10/21 06:53:38 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/21 06:53:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53086.\",\n",
       " '15/10/21 06:53:38 INFO NettyBlockTransferService: Server created on 53086',\n",
       " '15/10/21 06:53:38 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/21 06:53:38 INFO BlockManagerMasterEndpoint: Registering block manager localhost:53086 with 265.4 MB RAM, BlockManagerId(driver, localhost, 53086)',\n",
       " '15/10/21 06:53:38 INFO BlockManagerMaster: Registered BlockManager',\n",
       " '[I 06:53:41.044 NotebookApp] Saving file at /RedRock Update.ipynb',\n",
       " '[I 06:54:50.968 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " '[I 06:54:50.969 NotebookApp] Provisioning local kernel: python2',\n",
       " '[I 06:54:51.014 NotebookApp] Kernel started: cdc7d048-052d-4fbb-b8ff-a143678d3fb7',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/21 06:54:54 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/21 06:54:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/21 06:54:54 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/21 06:54:54 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/21 06:54:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/21 06:54:55 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/21 06:54:55 INFO Remoting: Starting remoting',\n",
       " '15/10/21 06:54:55 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:34793]',\n",
       " \"15/10/21 06:54:55 INFO Utils: Successfully started service 'sparkDriver' on port 34793.\",\n",
       " '15/10/21 06:54:55 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/21 06:54:55 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/21 06:54:55 INFO DiskBlockManager: Created local directory at /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb/blockmgr-c7cde9d3-876b-4def-8d6b-103aab7c5654',\n",
       " '15/10/21 06:54:55 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/21 06:54:56 INFO HttpFileServer: HTTP File server directory is /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb/httpd-7dd197ab-d78a-45df-a99f-0cdd16edd456',\n",
       " '15/10/21 06:54:56 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/21 06:54:56 INFO Utils: Successfully started service 'HTTP file server' on port 44342.\",\n",
       " '15/10/21 06:54:56 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/21 06:54:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
       " \"15/10/21 06:54:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\",\n",
       " \"15/10/21 06:54:56 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\",\n",
       " \"15/10/21 06:54:56 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\",\n",
       " \"15/10/21 06:54:56 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\",\n",
       " \"15/10/21 06:54:56 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\",\n",
       " \"15/10/21 06:54:56 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\",\n",
       " \"15/10/21 06:54:56 INFO Utils: Successfully started service 'SparkUI' on port 4047.\",\n",
       " '15/10/21 06:54:56 INFO SparkUI: Started SparkUI at http://172.17.0.22:4047',\n",
       " '15/10/21 06:54:56 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/21 06:54:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50977.\",\n",
       " '15/10/21 06:54:57 INFO NettyBlockTransferService: Server created on 50977',\n",
       " '15/10/21 06:54:57 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/21 06:54:57 INFO BlockManagerMasterEndpoint: Registering block manager localhost:50977 with 265.4 MB RAM, BlockManagerId(driver, localhost, 50977)',\n",
       " '15/10/21 06:54:57 INFO BlockManagerMaster: Registered BlockManager',\n",
       " '[I 06:54:59.513 NotebookApp] Request shutdown_kernel: cdc7d048-052d-4fbb-b8ff-a143678d3fb7, <IPython.kernel.ioloop.manager.IOLoopKernelManager object at 0x7fd6317cb290>',\n",
       " '15/10/21 06:54:59 INFO SparkUI: Stopped Spark web UI at http://172.17.0.22:4047',\n",
       " '15/10/21 06:54:59 INFO DAGScheduler: Stopping DAGScheduler',\n",
       " '15/10/21 06:54:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!',\n",
       " '15/10/21 06:54:59 INFO Utils: path = /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb/blockmgr-c7cde9d3-876b-4def-8d6b-103aab7c5654, already present as root for deletion.',\n",
       " '15/10/21 06:54:59 INFO MemoryStore: MemoryStore cleared',\n",
       " '15/10/21 06:54:59 INFO BlockManager: BlockManager stopped',\n",
       " '15/10/21 06:54:59 INFO BlockManagerMaster: BlockManagerMaster stopped',\n",
       " '15/10/21 06:54:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!',\n",
       " '15/10/21 06:54:59 INFO SparkContext: Successfully stopped SparkContext',\n",
       " '15/10/21 06:54:59 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.',\n",
       " '15/10/21 06:54:59 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.',\n",
       " '15/10/21 06:54:59 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.',\n",
       " '15/10/21 06:55:00 INFO Utils: Shutdown hook called',\n",
       " '15/10/21 06:55:00 INFO Utils: Deleting directory /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb',\n",
       " '[I 06:55:00.524 NotebookApp] Kernel shutdown: cdc7d048-052d-4fbb-b8ff-a143678d3fb7',\n",
       " '[I 06:55:05.136 NotebookApp] Saving file at /RedRock New Visualizations.ipynb',\n",
       " '[W 06:55:15.252 NotebookApp] Notebook RedRock New Visualizations0.ipynb is not trusted',\n",
       " '[I 06:55:15.489 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " '[I 06:55:15.489 NotebookApp] Provisioning local kernel: python2',\n",
       " '[I 06:55:15.537 NotebookApp] Kernel started: 80963995-4989-4617-8e7b-32071eb71849',\n",
       " 'WARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " '15/10/21 06:55:18 INFO SparkContext: Running Spark version 1.4.1',\n",
       " '15/10/21 06:55:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " '15/10/21 06:55:19 INFO SecurityManager: Changing view acls to: notebook',\n",
       " '15/10/21 06:55:19 INFO SecurityManager: Changing modify acls to: notebook',\n",
       " '15/10/21 06:55:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(notebook); users with modify permissions: Set(notebook)',\n",
       " '15/10/21 06:55:20 INFO Slf4jLogger: Slf4jLogger started',\n",
       " '15/10/21 06:55:20 INFO Remoting: Starting remoting',\n",
       " '15/10/21 06:55:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:53265]',\n",
       " \"15/10/21 06:55:20 INFO Utils: Successfully started service 'sparkDriver' on port 53265.\",\n",
       " '15/10/21 06:55:20 INFO SparkEnv: Registering MapOutputTracker',\n",
       " '15/10/21 06:55:20 INFO SparkEnv: Registering BlockManagerMaster',\n",
       " '15/10/21 06:55:20 INFO DiskBlockManager: Created local directory at /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/blockmgr-80f6b3de-e725-45a4-8df9-34bf5a8b291e',\n",
       " '15/10/21 06:55:20 INFO MemoryStore: MemoryStore started with capacity 265.4 MB',\n",
       " '15/10/21 06:55:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/httpd-a749eee4-3bc1-429a-94d0-d221f2f3738a',\n",
       " '15/10/21 06:55:20 INFO HttpServer: Starting HTTP Server',\n",
       " \"15/10/21 06:55:20 INFO Utils: Successfully started service 'HTTP file server' on port 50410.\",\n",
       " '15/10/21 06:55:20 INFO SparkEnv: Registering OutputCommitCoordinator',\n",
       " \"15/10/21 06:55:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\",\n",
       " \"15/10/21 06:55:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\",\n",
       " \"15/10/21 06:55:21 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\",\n",
       " \"15/10/21 06:55:21 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\",\n",
       " \"15/10/21 06:55:21 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\",\n",
       " \"15/10/21 06:55:21 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\",\n",
       " \"15/10/21 06:55:21 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\",\n",
       " \"15/10/21 06:55:21 INFO Utils: Successfully started service 'SparkUI' on port 4047.\",\n",
       " '15/10/21 06:55:21 INFO SparkUI: Started SparkUI at http://172.17.0.22:4047',\n",
       " '15/10/21 06:55:21 INFO Executor: Starting executor ID driver on host localhost',\n",
       " \"15/10/21 06:55:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45478.\",\n",
       " '15/10/21 06:55:21 INFO NettyBlockTransferService: Server created on 45478',\n",
       " '15/10/21 06:55:21 INFO BlockManagerMaster: Trying to register BlockManager',\n",
       " '15/10/21 06:55:21 INFO BlockManagerMasterEndpoint: Registering block manager localhost:45478 with 265.4 MB RAM, BlockManagerId(driver, localhost, 45478)',\n",
       " '15/10/21 06:55:21 INFO BlockManagerMaster: Registered BlockManager',\n",
       " '[I 06:55:22.523 NotebookApp] Request shutdown_kernel: 80963995-4989-4617-8e7b-32071eb71849, <IPython.kernel.ioloop.manager.IOLoopKernelManager object at 0x7fd6317d2dd0>',\n",
       " '15/10/21 06:55:22 INFO SparkUI: Stopped Spark web UI at http://172.17.0.22:4047',\n",
       " '15/10/21 06:55:22 INFO DAGScheduler: Stopping DAGScheduler',\n",
       " '15/10/21 06:55:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!',\n",
       " '15/10/21 06:55:22 INFO Utils: path = /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/blockmgr-80f6b3de-e725-45a4-8df9-34bf5a8b291e, already present as root for deletion.',\n",
       " '15/10/21 06:55:22 INFO MemoryStore: MemoryStore cleared',\n",
       " '15/10/21 06:55:22 INFO BlockManager: BlockManager stopped',\n",
       " '15/10/21 06:55:22 INFO BlockManagerMaster: BlockManagerMaster stopped',\n",
       " '15/10/21 06:55:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!',\n",
       " '15/10/21 06:55:22 INFO SparkContext: Successfully stopped SparkContext',\n",
       " '15/10/21 06:55:22 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.',\n",
       " '15/10/21 06:55:22 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.',\n",
       " '15/10/21 06:55:22 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.',\n",
       " '[I 06:55:23.536 NotebookApp] Kernel shutdown: 80963995-4989-4617-8e7b-32071eb71849',\n",
       " '15/10/21 06:55:23 INFO Utils: Shutdown hook called',\n",
       " '15/10/21 06:55:23 INFO Utils: Deleting directory /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/pyspark-6db0c8fd-094a-4f08-bd68-dff219e65350',\n",
       " '15/10/21 06:55:23 INFO Utils: Deleting directory /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a',\n",
       " '[W 06:55:34.748 NotebookApp] Notebook RedRock New Visualizations.ipynb is not trusted',\n",
       " '[I 06:55:35.016 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=python2, path=\"\", **kwargs={}',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34836"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13438"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out the lines that contains INFO\n",
    "info = logFile.filter(lambda line: \"INFO\" in line)\n",
    "info.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the lines with \"spark\" in it by combining transformation and action.\n",
    "info.filter(lambda line: \"spark\" in line).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15/10/14 14:29:23 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:53333]',\n",
       " \"15/10/14 14:29:23 INFO Utils: Successfully started service 'sparkDriver' on port 53333.\",\n",
       " '15/10/14 14:29:23 INFO DiskBlockManager: Created local directory at /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6/blockmgr-c142f2f1-ebb6-4612-945b-0a67d156230a',\n",
       " '15/10/14 14:29:23 INFO HttpFileServer: HTTP File server directory is /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6/httpd-ed3f4ab0-7218-48bc-9d8a-3981b1cfe574',\n",
       " \"15/10/14 14:29:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35726.\",\n",
       " '15/10/15 15:33:42 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:47412]',\n",
       " \"15/10/15 15:33:42 INFO Utils: Successfully started service 'sparkDriver' on port 47412.\",\n",
       " '15/10/15 15:33:42 INFO DiskBlockManager: Created local directory at /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46/blockmgr-aad4e583-6a6c-479a-b021-a7e0390ea261',\n",
       " '15/10/15 15:33:42 INFO HttpFileServer: HTTP File server directory is /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46/httpd-80730048-1dcb-4da2-8458-8bf3eba96046',\n",
       " \"15/10/15 15:33:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47915.\",\n",
       " '15/10/16 13:08:23 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:58378]',\n",
       " \"15/10/16 13:08:23 INFO Utils: Successfully started service 'sparkDriver' on port 58378.\",\n",
       " '15/10/16 13:08:23 INFO DiskBlockManager: Created local directory at /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/blockmgr-24ee1e5a-9311-4665-8ce7-56fc1f0601a0',\n",
       " '15/10/16 13:08:23 INFO HttpFileServer: HTTP File server directory is /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/httpd-98632027-ee06-401b-a027-b7973b158023',\n",
       " \"15/10/16 13:08:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34420.\",\n",
       " '15/10/16 13:13:22 INFO Utils: path = /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/blockmgr-24ee1e5a-9311-4665-8ce7-56fc1f0601a0, already present as root for deletion.',\n",
       " '15/10/16 13:13:22 INFO Utils: Deleting directory /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/pyspark-2107622a-f8ad-4b5a-b456-f0e414fbed40',\n",
       " '15/10/16 13:13:22 INFO Utils: Deleting directory /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70',\n",
       " '15/10/16 13:13:27 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:38668]',\n",
       " \"15/10/16 13:13:27 INFO Utils: Successfully started service 'sparkDriver' on port 38668.\",\n",
       " '15/10/16 13:13:27 INFO DiskBlockManager: Created local directory at /tmp/spark-52bdb6b1-7781-4abd-9758-bfc0d2a578ec/blockmgr-e0992345-e860-44ea-aaca-50e75bd99684',\n",
       " '15/10/16 13:13:27 INFO HttpFileServer: HTTP File server directory is /tmp/spark-52bdb6b1-7781-4abd-9758-bfc0d2a578ec/httpd-fe5e1d28-9663-460b-97fd-e2374b912583',\n",
       " \"15/10/16 13:13:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44407.\",\n",
       " '15/10/16 14:52:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:43750]',\n",
       " \"15/10/16 14:52:20 INFO Utils: Successfully started service 'sparkDriver' on port 43750.\",\n",
       " '15/10/16 14:52:20 INFO DiskBlockManager: Created local directory at /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4/blockmgr-73dbe021-6e2b-43f9-9547-72004cf3a221',\n",
       " '15/10/16 14:52:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4/httpd-a9ac31c5-fdd1-4437-a29f-771847924c71',\n",
       " \"15/10/16 14:52:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54796.\",\n",
       " '15/10/21 06:09:21 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:43928]',\n",
       " \"15/10/21 06:09:21 INFO Utils: Successfully started service 'sparkDriver' on port 43928.\",\n",
       " '15/10/21 06:09:21 INFO DiskBlockManager: Created local directory at /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a/blockmgr-ed90f7d7-7049-471a-8560-950825742016',\n",
       " '15/10/21 06:09:21 INFO HttpFileServer: HTTP File server directory is /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a/httpd-3f3cd3ee-81f2-4ba5-be62-ccdb6d62cf52',\n",
       " \"15/10/21 06:09:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34705.\",\n",
       " '15/10/21 06:18:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:34767]',\n",
       " \"15/10/21 06:18:20 INFO Utils: Successfully started service 'sparkDriver' on port 34767.\",\n",
       " '15/10/21 06:18:20 INFO DiskBlockManager: Created local directory at /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350/blockmgr-5abb109b-5c36-4d13-ac93-7ad13e807555',\n",
       " '15/10/21 06:18:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350/httpd-81687cf4-f5a6-4a97-8e52-a6096ad60235',\n",
       " \"15/10/21 06:18:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58989.\",\n",
       " '15/10/21 06:44:27 INFO Utils: path = /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46/blockmgr-aad4e583-6a6c-479a-b021-a7e0390ea261, already present as root for deletion.',\n",
       " '15/10/21 06:44:27 INFO Utils: Deleting directory /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46',\n",
       " '15/10/21 06:44:41 INFO Utils: path = /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a/blockmgr-ed90f7d7-7049-471a-8560-950825742016, already present as root for deletion.',\n",
       " '15/10/21 06:44:42 INFO Utils: Deleting directory /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a',\n",
       " '15/10/21 06:46:03 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:44681]',\n",
       " \"15/10/21 06:46:03 INFO Utils: Successfully started service 'sparkDriver' on port 44681.\",\n",
       " '15/10/21 06:46:03 INFO DiskBlockManager: Created local directory at /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5/blockmgr-719b62d4-5020-486a-bf06-ff030c696f62',\n",
       " '15/10/21 06:46:04 INFO HttpFileServer: HTTP File server directory is /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5/httpd-ca2b9527-9689-44df-90b9-94eb76bf22c8',\n",
       " \"15/10/21 06:46:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33807.\",\n",
       " '15/10/21 06:46:06 INFO Utils: path = /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5/blockmgr-719b62d4-5020-486a-bf06-ff030c696f62, already present as root for deletion.',\n",
       " '15/10/21 06:46:06 INFO Utils: Deleting directory /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5',\n",
       " '15/10/21 06:46:18 INFO Utils: path = /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350/blockmgr-5abb109b-5c36-4d13-ac93-7ad13e807555, already present as root for deletion.',\n",
       " '15/10/21 06:46:19 INFO Utils: Deleting directory /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350',\n",
       " '15/10/21 06:46:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:34749]',\n",
       " \"15/10/21 06:46:20 INFO Utils: Successfully started service 'sparkDriver' on port 34749.\",\n",
       " '15/10/21 06:46:20 INFO DiskBlockManager: Created local directory at /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/blockmgr-fb8c79d9-cb49-4e14-9eae-02211819594f',\n",
       " '15/10/21 06:46:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/httpd-72d3e35d-a6b4-427b-b7cd-7f40b45041ae',\n",
       " \"15/10/21 06:46:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43363.\",\n",
       " '15/10/21 06:51:44 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:58291]',\n",
       " \"15/10/21 06:51:44 INFO Utils: Successfully started service 'sparkDriver' on port 58291.\",\n",
       " '15/10/21 06:51:44 INFO DiskBlockManager: Created local directory at /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5/blockmgr-22d08b29-3ede-43e5-b659-7938c320c115',\n",
       " '15/10/21 06:51:44 INFO HttpFileServer: HTTP File server directory is /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5/httpd-c3af5d8f-93b1-4ea1-b4cd-d939821a87ee',\n",
       " \"15/10/21 06:51:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60134.\",\n",
       " '15/10/21 06:53:15 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:52949]',\n",
       " \"15/10/21 06:53:15 INFO Utils: Successfully started service 'sparkDriver' on port 52949.\",\n",
       " '15/10/21 06:53:15 INFO DiskBlockManager: Created local directory at /tmp/spark-a0d30f27-58f2-4803-b7ce-2f437dce18c1/blockmgr-33399bc4-6281-42c6-b023-b7bcd4a56bc2',\n",
       " '15/10/21 06:53:15 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a0d30f27-58f2-4803-b7ce-2f437dce18c1/httpd-0637bf42-85e3-45a9-a395-9fadcb6744a4',\n",
       " \"15/10/21 06:53:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42148.\",\n",
       " '15/10/21 06:53:37 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:48625]',\n",
       " \"15/10/21 06:53:37 INFO Utils: Successfully started service 'sparkDriver' on port 48625.\",\n",
       " '15/10/21 06:53:37 INFO DiskBlockManager: Created local directory at /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3/blockmgr-822dc396-71cd-4fe2-893d-9f536687422a',\n",
       " '15/10/21 06:53:37 INFO HttpFileServer: HTTP File server directory is /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3/httpd-034f2b34-e002-40b1-9500-9409076170ec',\n",
       " \"15/10/21 06:53:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53086.\",\n",
       " '15/10/21 06:54:55 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:34793]',\n",
       " \"15/10/21 06:54:55 INFO Utils: Successfully started service 'sparkDriver' on port 34793.\",\n",
       " '15/10/21 06:54:55 INFO DiskBlockManager: Created local directory at /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb/blockmgr-c7cde9d3-876b-4def-8d6b-103aab7c5654',\n",
       " '15/10/21 06:54:56 INFO HttpFileServer: HTTP File server directory is /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb/httpd-7dd197ab-d78a-45df-a99f-0cdd16edd456',\n",
       " \"15/10/21 06:54:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50977.\",\n",
       " '15/10/21 06:54:59 INFO Utils: path = /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb/blockmgr-c7cde9d3-876b-4def-8d6b-103aab7c5654, already present as root for deletion.',\n",
       " '15/10/21 06:55:00 INFO Utils: Deleting directory /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb',\n",
       " '15/10/21 06:55:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:53265]',\n",
       " \"15/10/21 06:55:20 INFO Utils: Successfully started service 'sparkDriver' on port 53265.\",\n",
       " '15/10/21 06:55:20 INFO DiskBlockManager: Created local directory at /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/blockmgr-80f6b3de-e725-45a4-8df9-34bf5a8b291e',\n",
       " '15/10/21 06:55:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/httpd-a749eee4-3bc1-429a-94d0-d221f2f3738a',\n",
       " \"15/10/21 06:55:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45478.\",\n",
       " '15/10/21 06:55:22 INFO Utils: path = /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/blockmgr-80f6b3de-e725-45a4-8df9-34bf5a8b291e, already present as root for deletion.',\n",
       " '15/10/21 06:55:23 INFO Utils: Deleting directory /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/pyspark-6db0c8fd-094a-4f08-bd68-dff219e65350',\n",
       " '15/10/21 06:55:23 INFO Utils: Deleting directory /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a',\n",
       " '15/10/21 07:14:56 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:45827]',\n",
       " \"15/10/21 07:14:56 INFO Utils: Successfully started service 'sparkDriver' on port 45827.\",\n",
       " '15/10/21 07:14:56 INFO DiskBlockManager: Created local directory at /tmp/spark-0f1efbbf-1336-4e53-b9e1-c3a0b791b808/blockmgr-573c0859-eb51-466c-99b6-84c3311f512c',\n",
       " '15/10/21 07:14:56 INFO HttpFileServer: HTTP File server directory is /tmp/spark-0f1efbbf-1336-4e53-b9e1-c3a0b791b808/httpd-4a3e389d-7784-4587-95d4-46cd1d001fca',\n",
       " \"15/10/21 07:14:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53959.\",\n",
       " '15/10/21 07:56:30 [INFO] Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:39281]',\n",
       " '15/10/21 15:43:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:39173]',\n",
       " \"15/10/21 15:43:57 INFO Utils: Successfully started service 'sparkDriver' on port 39173.\",\n",
       " '15/10/21 15:43:57 INFO DiskBlockManager: Created local directory at /tmp/spark-6e54b9ef-eeee-4cb9-bdd3-c91f0bfba6e8/blockmgr-586c41e5-996a-4a68-87a5-5b736a9618b6',\n",
       " '15/10/21 15:43:57 INFO HttpFileServer: HTTP File server directory is /tmp/spark-6e54b9ef-eeee-4cb9-bdd3-c91f0bfba6e8/httpd-4b254c9b-ffb8-4603-bf57-49661e22248d',\n",
       " \"15/10/21 15:43:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54908.\",\n",
       " '15/10/21 17:02:06 INFO Utils: path = /tmp/spark-6e54b9ef-eeee-4cb9-bdd3-c91f0bfba6e8/blockmgr-586c41e5-996a-4a68-87a5-5b736a9618b6, already present as root for deletion.',\n",
       " '15/10/21 17:02:07 INFO Utils: Deleting directory /tmp/spark-6e54b9ef-eeee-4cb9-bdd3-c91f0bfba6e8',\n",
       " '15/10/21 17:02:12 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:42749]',\n",
       " \"15/10/21 17:02:12 INFO Utils: Successfully started service 'sparkDriver' on port 42749.\",\n",
       " '15/10/21 17:02:12 INFO DiskBlockManager: Created local directory at /tmp/spark-dc939ed3-9bec-4751-8e83-df927a97a500/blockmgr-264e8036-5ed0-4b03-b896-6ff04e27f572',\n",
       " '15/10/21 17:02:12 INFO HttpFileServer: HTTP File server directory is /tmp/spark-dc939ed3-9bec-4751-8e83-df927a97a500/httpd-fc7a9c70-76bc-4605-958f-e115bd3e8d47',\n",
       " \"15/10/21 17:02:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45162.\",\n",
       " '15/10/22 02:32:26 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:44439]',\n",
       " \"15/10/22 02:32:26 INFO Utils: Successfully started service 'sparkDriver' on port 44439.\",\n",
       " '15/10/22 02:32:26 INFO DiskBlockManager: Created local directory at /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213/blockmgr-ff3b1fac-22e5-4969-8e3a-2aecbf2c0dcc',\n",
       " '15/10/22 02:32:26 INFO HttpFileServer: HTTP File server directory is /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213/httpd-af60c2aa-69b8-4878-86e3-43b8fccdb6ac',\n",
       " \"15/10/22 02:32:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59251.\",\n",
       " '15/10/22 04:36:32 INFO Utils: path = /tmp/spark-dc939ed3-9bec-4751-8e83-df927a97a500/blockmgr-264e8036-5ed0-4b03-b896-6ff04e27f572, already present as root for deletion.',\n",
       " '15/10/22 04:36:32 INFO Utils: Deleting directory /tmp/spark-dc939ed3-9bec-4751-8e83-df927a97a500',\n",
       " '15/10/22 04:36:37 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:57259]',\n",
       " \"15/10/22 04:36:37 INFO Utils: Successfully started service 'sparkDriver' on port 57259.\",\n",
       " '15/10/22 04:36:37 INFO DiskBlockManager: Created local directory at /tmp/spark-6ffd920e-2dd5-43d4-a2b8-6b3c3a1ae0c7/blockmgr-f28c1757-42ed-4495-bca7-f4693f2f1846',\n",
       " '15/10/22 04:36:38 INFO HttpFileServer: HTTP File server directory is /tmp/spark-6ffd920e-2dd5-43d4-a2b8-6b3c3a1ae0c7/httpd-8bda8744-f6f5-481c-9dd9-065b7bf0f7b9',\n",
       " \"15/10/22 04:36:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34923.\",\n",
       " '15/10/22 05:42:05 INFO Utils: path = /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4/blockmgr-73dbe021-6e2b-43f9-9547-72004cf3a221, already present as root for deletion.',\n",
       " '15/10/22 05:42:06 INFO Utils: Deleting directory /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4',\n",
       " '15/10/22 05:42:59 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:32997]',\n",
       " \"15/10/22 05:42:59 INFO Utils: Successfully started service 'sparkDriver' on port 32997.\",\n",
       " '15/10/22 05:42:59 INFO DiskBlockManager: Created local directory at /tmp/spark-cf71c18f-eb1b-4344-8525-a2574c650a59/blockmgr-6da140df-5530-460e-a154-b780fb3839ff',\n",
       " '15/10/22 05:43:00 INFO HttpFileServer: HTTP File server directory is /tmp/spark-cf71c18f-eb1b-4344-8525-a2574c650a59/httpd-5a4d1c53-dd5f-4d13-8f82-fb56ecc67896',\n",
       " \"15/10/22 05:43:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39140.\",\n",
       " '15/10/22 05:44:02 INFO Utils: path = /tmp/spark-cf71c18f-eb1b-4344-8525-a2574c650a59/blockmgr-6da140df-5530-460e-a154-b780fb3839ff, already present as root for deletion.',\n",
       " '15/10/22 05:44:03 INFO Utils: Deleting directory /tmp/spark-cf71c18f-eb1b-4344-8525-a2574c650a59',\n",
       " '15/10/22 05:44:23 INFO Utils: path = /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3/blockmgr-822dc396-71cd-4fe2-893d-9f536687422a, already present as root for deletion.',\n",
       " '15/10/22 05:44:24 INFO Utils: Deleting directory /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3',\n",
       " '15/10/22 05:44:31 INFO Utils: path = /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/blockmgr-fb8c79d9-cb49-4e14-9eae-02211819594f, already present as root for deletion.',\n",
       " '15/10/22 05:44:32 INFO Utils: Deleting directory /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/pyspark-9e9ff3f6-2e32-4570-ae7a-22a651278319',\n",
       " '15/10/22 05:44:32 INFO Utils: Deleting directory /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f',\n",
       " '15/10/22 05:44:42 INFO Utils: path = /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213/blockmgr-ff3b1fac-22e5-4969-8e3a-2aecbf2c0dcc, already present as root for deletion.',\n",
       " '15/10/22 05:44:43 INFO Utils: Deleting directory /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213/pyspark-00ef6c66-4db7-4741-b890-7647fc2d4f76',\n",
       " '15/10/22 05:44:43 INFO Utils: Deleting directory /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213',\n",
       " '15/10/22 05:44:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:46937]',\n",
       " \"15/10/22 05:44:57 INFO Utils: Successfully started service 'sparkDriver' on port 46937.\",\n",
       " '15/10/22 05:44:57 INFO DiskBlockManager: Created local directory at /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f/blockmgr-e7a8bf00-0701-4cb7-a835-b8448d9fe79c',\n",
       " '15/10/22 05:44:57 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f/httpd-91230dd5-3f29-4655-906e-228bc7bde472',\n",
       " \"15/10/22 05:44:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37166.\",\n",
       " '15/10/22 05:45:01 INFO Utils: path = /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f/blockmgr-e7a8bf00-0701-4cb7-a835-b8448d9fe79c, already present as root for deletion.',\n",
       " '15/10/22 05:45:02 INFO Utils: Deleting directory /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f/pyspark-c5a99eda-9137-401b-99a1-ffeae801f695',\n",
       " '15/10/22 05:45:02 INFO Utils: Deleting directory /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f',\n",
       " '15/10/22 05:48:07 INFO Utils: path = /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5/blockmgr-22d08b29-3ede-43e5-b659-7938c320c115, already present as root for deletion.',\n",
       " '15/10/22 05:48:08 INFO Utils: Deleting directory /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5',\n",
       " '15/10/22 06:12:32 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:56255]',\n",
       " \"15/10/22 06:12:32 INFO Utils: Successfully started service 'sparkDriver' on port 56255.\",\n",
       " '15/10/22 06:12:32 INFO DiskBlockManager: Created local directory at /tmp/spark-7ca4125c-fd13-4197-aa7f-9fe6163feca2/blockmgr-88b233e4-cd52-4810-b90f-fd20425e41c4',\n",
       " '15/10/22 06:12:32 INFO HttpFileServer: HTTP File server directory is /tmp/spark-7ca4125c-fd13-4197-aa7f-9fe6163feca2/httpd-0f8a2ab5-bb96-4597-98e9-db0d62952c1f',\n",
       " \"15/10/22 06:12:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34717.\",\n",
       " '15/10/22 06:33:24 INFO Utils: path = /tmp/spark-7ca4125c-fd13-4197-aa7f-9fe6163feca2/blockmgr-88b233e4-cd52-4810-b90f-fd20425e41c4, already present as root for deletion.',\n",
       " '15/10/22 06:33:25 INFO Utils: Deleting directory /tmp/spark-7ca4125c-fd13-4197-aa7f-9fe6163feca2/pyspark-7fef74de-62da-4241-a21b-35d6b6075968',\n",
       " '15/10/22 06:33:25 INFO Utils: Deleting directory /tmp/spark-7ca4125c-fd13-4197-aa7f-9fe6163feca2',\n",
       " '15/10/22 06:33:30 INFO Utils: path = /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6/blockmgr-c142f2f1-ebb6-4612-945b-0a67d156230a, already present as root for deletion.',\n",
       " '15/10/22 06:33:30 INFO Utils: Deleting directory /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6',\n",
       " '15/10/22 06:33:35 INFO Utils: path = /tmp/spark-0f1efbbf-1336-4e53-b9e1-c3a0b791b808/blockmgr-573c0859-eb51-466c-99b6-84c3311f512c, already present as root for deletion.',\n",
       " '15/10/22 06:33:35 INFO Utils: Deleting directory /tmp/spark-0f1efbbf-1336-4e53-b9e1-c3a0b791b808/pyspark-56ef90f8-2b4f-43b5-a7e4-219c544e948e',\n",
       " '15/10/22 06:33:35 INFO Utils: Deleting directory /tmp/spark-0f1efbbf-1336-4e53-b9e1-c3a0b791b808']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch those lines as an array of Strings\n",
    "info.filter(lambda line: \"spark\" in line).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(2) PythonRDD[8] at RDD at PythonRDD.scala:53 []\\n |  /home/jane/Downloads/LabData/notebook.log MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\\n |  /home/jane/Downloads/LabData/notebook.log HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []'\n"
     ]
    }
   ],
   "source": [
    "# View the graph of an RDD using this command:\n",
    "print(info.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Joining RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDDs for the same README and the POM files\n",
    "readmeFile = sc.textFile(\"/home/jane/Downloads/LabData/README.md\")\n",
    "pomFile = sc.textFile(\"/home/jane/Downloads/LabData/pom.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(readmeFile.filter(lambda line: \"Spark\" in line).count())\n",
    "print(pomFile.filter(lambda line: \"Spark\" in line).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do a WordCount on each RDD so that the results are (K,V) pairs of (word,count)\n",
    "wordCountREADME = readmeFile.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountPOM = pomFile.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or\n",
    "readmeCount = readmeFile.                    \\\n",
    "    flatMap(lambda line: line.split(\"   \")).   \\\n",
    "    map(lambda word: (word, 1)).             \\\n",
    "    reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "pomCount = pomFile.                          \\\n",
    "    flatMap(lambda line: line.split(\"   \")).   \\\n",
    "    map(lambda word: (word, 1)).            \\\n",
    "    reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1),\n",
       " ('Apache', 1),\n",
       " ('Spark', 14),\n",
       " ('is', 6),\n",
       " ('It', 2),\n",
       " ('provides', 1),\n",
       " ('high-level', 1),\n",
       " ('APIs', 1),\n",
       " ('in', 5),\n",
       " ('Scala,', 1),\n",
       " ('Java,', 1),\n",
       " ('an', 3),\n",
       " ('optimized', 1),\n",
       " ('engine', 1),\n",
       " ('supports', 2),\n",
       " ('computation', 1),\n",
       " ('analysis.', 1),\n",
       " ('set', 2),\n",
       " ('of', 5),\n",
       " ('tools', 1),\n",
       " ('SQL', 2),\n",
       " ('MLlib', 1),\n",
       " ('machine', 1),\n",
       " ('learning,', 1),\n",
       " ('GraphX', 1),\n",
       " ('graph', 1),\n",
       " ('processing,', 1),\n",
       " ('Documentation', 1),\n",
       " ('latest', 1),\n",
       " ('programming', 1),\n",
       " ('guide,', 1),\n",
       " ('[project', 2),\n",
       " ('README', 1),\n",
       " ('only', 1),\n",
       " ('basic', 1),\n",
       " ('instructions.', 1),\n",
       " ('Building', 1),\n",
       " ('using', 2),\n",
       " ('[Apache', 1),\n",
       " ('run:', 1),\n",
       " ('do', 2),\n",
       " ('this', 1),\n",
       " ('downloaded', 1),\n",
       " ('documentation', 3),\n",
       " ('project', 1),\n",
       " ('site,', 1),\n",
       " ('at', 2),\n",
       " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " ('Interactive', 2),\n",
       " ('Shell', 2),\n",
       " ('The', 1),\n",
       " ('way', 1),\n",
       " ('start', 1),\n",
       " ('Try', 1),\n",
       " ('following', 2),\n",
       " ('1000:', 2),\n",
       " ('scala>', 1),\n",
       " ('1000).count()', 1),\n",
       " ('Python', 2),\n",
       " ('Alternatively,', 1),\n",
       " ('use', 3),\n",
       " ('And', 1),\n",
       " ('run', 7),\n",
       " ('Example', 1),\n",
       " ('several', 1),\n",
       " ('programs', 2),\n",
       " ('them,', 1),\n",
       " ('`./bin/run-example', 1),\n",
       " ('[params]`.', 1),\n",
       " ('example:', 1),\n",
       " ('./bin/run-example', 2),\n",
       " ('SparkPi', 2),\n",
       " ('variable', 1),\n",
       " ('when', 1),\n",
       " ('examples', 2),\n",
       " ('spark://', 1),\n",
       " ('URL,', 1),\n",
       " ('YARN,', 1),\n",
       " ('\"local\"', 1),\n",
       " ('locally', 2),\n",
       " ('N', 1),\n",
       " ('abbreviated', 1),\n",
       " ('class', 2),\n",
       " ('name', 1),\n",
       " ('package.', 1),\n",
       " ('instance:', 1),\n",
       " ('print', 1),\n",
       " ('usage', 1),\n",
       " ('help', 1),\n",
       " ('no', 1),\n",
       " ('params', 1),\n",
       " ('are', 1),\n",
       " ('Testing', 1),\n",
       " ('Spark](#building-spark).', 1),\n",
       " ('Once', 1),\n",
       " ('built,', 1),\n",
       " ('tests', 2),\n",
       " ('using:', 1),\n",
       " ('./dev/run-tests', 1),\n",
       " ('Please', 3),\n",
       " ('guidance', 3),\n",
       " ('module,', 1),\n",
       " ('individual', 1),\n",
       " ('Note', 1),\n",
       " ('About', 1),\n",
       " ('uses', 1),\n",
       " ('library', 1),\n",
       " ('HDFS', 1),\n",
       " ('other', 1),\n",
       " ('Hadoop-supported', 1),\n",
       " ('storage', 1),\n",
       " ('systems.', 1),\n",
       " ('Because', 1),\n",
       " ('have', 1),\n",
       " ('changed', 1),\n",
       " ('different', 1),\n",
       " ('versions', 1),\n",
       " ('Hadoop,', 2),\n",
       " ('must', 1),\n",
       " ('against', 1),\n",
       " ('version', 1),\n",
       " ('refer', 2),\n",
       " ('particular', 3),\n",
       " ('distribution', 1),\n",
       " ('Hive', 2),\n",
       " ('Thriftserver', 1),\n",
       " ('distributions.', 1),\n",
       " ('[\"Third', 1),\n",
       " ('distribution.', 1),\n",
       " ('[Configuration', 1),\n",
       " ('Guide](http://spark.apache.org/docs/latest/configuration.html)', 1),\n",
       " ('online', 1),\n",
       " ('overview', 1),\n",
       " ('configure', 1),\n",
       " ('Spark.', 1),\n",
       " ('a', 10),\n",
       " ('fast', 1),\n",
       " ('and', 10),\n",
       " ('general', 2),\n",
       " ('cluster', 2),\n",
       " ('computing', 1),\n",
       " ('system', 1),\n",
       " ('for', 12),\n",
       " ('Big', 1),\n",
       " ('Data.', 1),\n",
       " ('Python,', 2),\n",
       " ('R,', 1),\n",
       " ('that', 3),\n",
       " ('graphs', 1),\n",
       " ('data', 1),\n",
       " ('also', 5),\n",
       " ('rich', 1),\n",
       " ('higher-level', 1),\n",
       " ('including', 3),\n",
       " ('DataFrames,', 1),\n",
       " ('Streaming', 1),\n",
       " ('stream', 1),\n",
       " ('processing.', 1),\n",
       " ('<http://spark.apache.org/>', 1),\n",
       " ('##', 8),\n",
       " ('Online', 1),\n",
       " ('You', 3),\n",
       " ('can', 6),\n",
       " ('find', 1),\n",
       " ('the', 21),\n",
       " ('documentation,', 1),\n",
       " ('on', 6),\n",
       " ('web', 1),\n",
       " ('page](http://spark.apache.org/documentation.html)', 1),\n",
       " ('wiki](https://cwiki.apache.org/confluence/display/SPARK).', 1),\n",
       " ('This', 2),\n",
       " ('file', 1),\n",
       " ('contains', 1),\n",
       " ('setup', 1),\n",
       " ('built', 1),\n",
       " ('Maven](http://maven.apache.org/).', 1),\n",
       " ('To', 2),\n",
       " ('build', 3),\n",
       " ('its', 1),\n",
       " ('example', 3),\n",
       " ('programs,', 1),\n",
       " ('build/mvn', 1),\n",
       " ('-DskipTests', 1),\n",
       " ('clean', 1),\n",
       " ('package', 1),\n",
       " ('(You', 1),\n",
       " ('not', 1),\n",
       " ('need', 1),\n",
       " ('to', 14),\n",
       " ('if', 4),\n",
       " ('you', 4),\n",
       " ('pre-built', 1),\n",
       " ('package.)', 1),\n",
       " ('More', 1),\n",
       " ('detailed', 2),\n",
       " ('available', 1),\n",
       " ('from', 1),\n",
       " ('[\"Building', 1),\n",
       " ('Scala', 2),\n",
       " ('easiest', 1),\n",
       " ('through', 1),\n",
       " ('shell:', 2),\n",
       " ('./bin/spark-shell', 1),\n",
       " ('command,', 2),\n",
       " ('which', 2),\n",
       " ('should', 2),\n",
       " ('return', 2),\n",
       " ('sc.parallelize(1', 1),\n",
       " ('prefer', 1),\n",
       " ('./bin/pyspark', 1),\n",
       " ('>>>', 1),\n",
       " ('sc.parallelize(range(1000)).count()', 1),\n",
       " ('Programs', 1),\n",
       " ('comes', 1),\n",
       " ('with', 4),\n",
       " ('sample', 1),\n",
       " ('`examples`', 2),\n",
       " ('directory.', 1),\n",
       " ('one', 2),\n",
       " ('<class>', 1),\n",
       " ('For', 2),\n",
       " ('will', 1),\n",
       " ('Pi', 1),\n",
       " ('locally.', 1),\n",
       " ('MASTER', 1),\n",
       " ('environment', 1),\n",
       " ('running', 1),\n",
       " ('submit', 1),\n",
       " ('cluster.', 1),\n",
       " ('be', 2),\n",
       " ('mesos://', 1),\n",
       " ('or', 3),\n",
       " ('\"yarn\"', 1),\n",
       " ('thread,', 1),\n",
       " ('\"local[N]\"', 1),\n",
       " ('threads.', 1),\n",
       " ('MASTER=spark://host:7077', 1),\n",
       " ('Many', 1),\n",
       " ('given.', 1),\n",
       " ('Running', 1),\n",
       " ('Tests', 1),\n",
       " ('first', 1),\n",
       " ('requires', 1),\n",
       " ('[building', 1),\n",
       " ('see', 1),\n",
       " ('how', 2),\n",
       " ('[run', 1),\n",
       " ('tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).',\n",
       "  1),\n",
       " ('A', 1),\n",
       " ('Hadoop', 4),\n",
       " ('Versions', 1),\n",
       " ('core', 1),\n",
       " ('talk', 1),\n",
       " ('protocols', 1),\n",
       " ('same', 1),\n",
       " ('your', 1),\n",
       " ('runs.', 1),\n",
       " ('[\"Specifying', 1),\n",
       " ('Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n",
       "  1),\n",
       " ('building', 3),\n",
       " ('See', 1),\n",
       " ('Party', 1),\n",
       " ('Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html)',\n",
       "  1),\n",
       " ('application', 1),\n",
       " ('works', 1),\n",
       " ('Configuration', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountREADME.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readme Count\n",
      "\n",
      "[('#', 1), ('Apache', 1), ('Spark', 14), ('is', 6), ('It', 2), ('provides', 1), ('high-level', 1), ('APIs', 1), ('in', 5), ('Scala,', 1), ('Java,', 1), ('an', 3), ('optimized', 1), ('engine', 1), ('supports', 2), ('computation', 1), ('analysis.', 1), ('set', 2), ('of', 5), ('tools', 1), ('SQL', 2), ('MLlib', 1), ('machine', 1), ('learning,', 1), ('GraphX', 1), ('graph', 1), ('processing,', 1), ('Documentation', 1), ('latest', 1), ('programming', 1), ('guide,', 1), ('[project', 2), ('README', 1), ('only', 1), ('basic', 1), ('instructions.', 1), ('Building', 1), ('using', 2), ('[Apache', 1), ('run:', 1), ('do', 2), ('this', 1), ('downloaded', 1), ('documentation', 3), ('project', 1), ('site,', 1), ('at', 2), ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1), ('Interactive', 2), ('Shell', 2), ('The', 1), ('way', 1), ('start', 1), ('Try', 1), ('following', 2), ('1000:', 2), ('scala>', 1), ('1000).count()', 1), ('Python', 2), ('Alternatively,', 1), ('use', 3), ('And', 1), ('run', 7), ('Example', 1), ('several', 1), ('programs', 2), ('them,', 1), ('`./bin/run-example', 1), ('[params]`.', 1), ('example:', 1), ('./bin/run-example', 2), ('SparkPi', 2), ('variable', 1), ('when', 1), ('examples', 2), ('spark://', 1), ('URL,', 1), ('YARN,', 1), ('\"local\"', 1), ('locally', 2), ('N', 1), ('abbreviated', 1), ('class', 2), ('name', 1), ('package.', 1), ('instance:', 1), ('print', 1), ('usage', 1), ('help', 1), ('no', 1), ('params', 1), ('are', 1), ('Testing', 1), ('Spark](#building-spark).', 1), ('Once', 1), ('built,', 1), ('tests', 2), ('using:', 1), ('./dev/run-tests', 1), ('Please', 3), ('guidance', 3), ('module,', 1), ('individual', 1), ('Note', 1), ('About', 1), ('uses', 1), ('library', 1), ('HDFS', 1), ('other', 1), ('Hadoop-supported', 1), ('storage', 1), ('systems.', 1), ('Because', 1), ('have', 1), ('changed', 1), ('different', 1), ('versions', 1), ('Hadoop,', 2), ('must', 1), ('against', 1), ('version', 1), ('refer', 2), ('particular', 3), ('distribution', 1), ('Hive', 2), ('Thriftserver', 1), ('distributions.', 1), ('[\"Third', 1), ('distribution.', 1), ('[Configuration', 1), ('Guide](http://spark.apache.org/docs/latest/configuration.html)', 1), ('online', 1), ('overview', 1), ('configure', 1), ('Spark.', 1), ('a', 10), ('fast', 1), ('and', 10), ('general', 2), ('cluster', 2), ('computing', 1), ('system', 1), ('for', 12), ('Big', 1), ('Data.', 1), ('Python,', 2), ('R,', 1), ('that', 3), ('graphs', 1), ('data', 1), ('also', 5), ('rich', 1), ('higher-level', 1), ('including', 3), ('DataFrames,', 1), ('Streaming', 1), ('stream', 1), ('processing.', 1), ('<http://spark.apache.org/>', 1), ('##', 8), ('Online', 1), ('You', 3), ('can', 6), ('find', 1), ('the', 21), ('documentation,', 1), ('on', 6), ('web', 1), ('page](http://spark.apache.org/documentation.html)', 1), ('wiki](https://cwiki.apache.org/confluence/display/SPARK).', 1), ('This', 2), ('file', 1), ('contains', 1), ('setup', 1), ('built', 1), ('Maven](http://maven.apache.org/).', 1), ('To', 2), ('build', 3), ('its', 1), ('example', 3), ('programs,', 1), ('build/mvn', 1), ('-DskipTests', 1), ('clean', 1), ('package', 1), ('(You', 1), ('not', 1), ('need', 1), ('to', 14), ('if', 4), ('you', 4), ('pre-built', 1), ('package.)', 1), ('More', 1), ('detailed', 2), ('available', 1), ('from', 1), ('[\"Building', 1), ('Scala', 2), ('easiest', 1), ('through', 1), ('shell:', 2), ('./bin/spark-shell', 1), ('command,', 2), ('which', 2), ('should', 2), ('return', 2), ('sc.parallelize(1', 1), ('prefer', 1), ('./bin/pyspark', 1), ('>>>', 1), ('sc.parallelize(range(1000)).count()', 1), ('Programs', 1), ('comes', 1), ('with', 4), ('sample', 1), ('`examples`', 2), ('directory.', 1), ('one', 2), ('<class>', 1), ('For', 2), ('will', 1), ('Pi', 1), ('locally.', 1), ('MASTER', 1), ('environment', 1), ('running', 1), ('submit', 1), ('cluster.', 1), ('be', 2), ('mesos://', 1), ('or', 3), ('\"yarn\"', 1), ('thread,', 1), ('\"local[N]\"', 1), ('threads.', 1), ('MASTER=spark://host:7077', 1), ('Many', 1), ('given.', 1), ('Running', 1), ('Tests', 1), ('first', 1), ('requires', 1), ('[building', 1), ('see', 1), ('how', 2), ('[run', 1), ('tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).', 1), ('A', 1), ('Hadoop', 4), ('Versions', 1), ('core', 1), ('talk', 1), ('protocols', 1), ('same', 1), ('your', 1), ('runs.', 1), ('[\"Specifying', 1), ('Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', 1), ('building', 3), ('See', 1), ('Party', 1), ('Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html)', 1), ('application', 1), ('works', 1), ('Configuration', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Or\n",
    "print(\"Readme Count\\n\")\n",
    "print(wordCountREADME.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<?xml', 1),\n",
       " ('version=\"1.0\"', 1),\n",
       " ('Apache', 2),\n",
       " ('more', 1),\n",
       " ('NOTICE', 1),\n",
       " ('this', 3),\n",
       " ('work', 1),\n",
       " ('additional', 1),\n",
       " ('regarding', 1),\n",
       " ('copyright', 1),\n",
       " ('The', 2),\n",
       " ('2.0', 1),\n",
       " ('(the', 1),\n",
       " ('\"License\");', 1),\n",
       " ('may', 2),\n",
       " ('use', 1),\n",
       " ('in', 3),\n",
       " ('compliance', 1),\n",
       " ('License.', 2),\n",
       " ('obtain', 1),\n",
       " ('of', 2),\n",
       " ('at', 1),\n",
       " ('law', 1),\n",
       " ('is', 2),\n",
       " ('an', 1),\n",
       " ('\"AS', 1),\n",
       " ('IS\"', 1),\n",
       " ('BASIS,', 1),\n",
       " ('CONDITIONS', 1),\n",
       " ('OF', 1),\n",
       " ('KIND,', 1),\n",
       " ('specific', 1),\n",
       " ('language', 1),\n",
       " ('limitations', 1),\n",
       " ('-->', 7),\n",
       " ('xmlns=\"http://maven.apache.org/POM/4.0.0\"', 1),\n",
       " ('http://maven.apache.org/xsd/maven-4.0.0.xsd\">', 1),\n",
       " ('<modelVersion>4.0.0</modelVersion>', 1),\n",
       " ('<artifactId>spark-parent_2.10</artifactId>', 1),\n",
       " ('<relativePath>../pom.xml</relativePath>', 1),\n",
       " ('</parent>', 1),\n",
       " ('<sbt.project.name>examples</sbt.project.name>', 1),\n",
       " ('Project', 1),\n",
       " ('Examples</name>', 1),\n",
       " ('<dependencies>', 2),\n",
       " ('<version>${project.version}</version>', 12),\n",
       " ('<artifactId>spark-streaming_${scala.binary.version}</artifactId>', 1),\n",
       " ('<artifactId>spark-bagel_${scala.binary.version}</artifactId>', 1),\n",
       " ('<artifactId>spark-hive_${scala.binary.version}</artifactId>', 1),\n",
       " ('<artifactId>spark-graphx_${scala.binary.version}</artifactId>', 1),\n",
       " ('<artifactId>spark-streaming-flume_${scala.binary.version}</artifactId>', 1),\n",
       " ('<exclusions>', 6),\n",
       " ('<exclusion>', 35),\n",
       " ('<groupId>org.spark-project.protobuf</groupId>', 1),\n",
       " ('</exclusion>', 35),\n",
       " ('SPARK-4455', 4),\n",
       " ('<artifactId>hbase-protocol</artifactId>', 1),\n",
       " ('<artifactId>hbase-common</artifactId>', 1),\n",
       " ('<groupId>io.netty</groupId>', 2),\n",
       " ('<artifactId>netty</artifactId>', 2),\n",
       " ('<groupId>org.apache.hadoop</groupId>', 7),\n",
       " ('<artifactId>hadoop-core</artifactId>', 1),\n",
       " ('<artifactId>hadoop-client</artifactId>', 1),\n",
       " ('<artifactId>hadoop-mapreduce-client-jobclient</artifactId>', 1),\n",
       " ('<artifactId>hadoop-auth</artifactId>', 1),\n",
       " ('<artifactId>hbase-hadoop1-compat</artifactId>', 1),\n",
       " ('<artifactId>commons-math</artifactId>', 1),\n",
       " ('<artifactId>jersey-core</artifactId>', 2),\n",
       " ('<groupId>org.slf4j</groupId>', 1),\n",
       " ('<artifactId>jersey-server</artifactId>', 1),\n",
       " ('<artifactId>jersey-json</artifactId>', 1),\n",
       " ('uses', 1),\n",
       " ('better,', 1),\n",
       " ('but', 1),\n",
       " ('<groupId>commons-io</groupId>', 1),\n",
       " ('<artifactId>commons-io</artifactId>', 1),\n",
       " ('<scope>test</scope>', 2),\n",
       " ('<artifactId>commons-math3</artifactId>', 2),\n",
       " ('<groupId>com.twitter</groupId>', 1),\n",
       " ('<groupId>org.scalacheck</groupId>', 1),\n",
       " ('<artifactId>cassandra-all</artifactId>', 1),\n",
       " ('<version>1.2.6</version>', 1),\n",
       " ('<artifactId>guava</artifactId>', 1),\n",
       " ('<groupId>com.googlecode.concurrentlinkedhashmap</groupId>', 1),\n",
       " ('<artifactId>concurrentlinkedhashmap-lru</artifactId>', 1),\n",
       " ('<artifactId>commons-cli</artifactId>', 1),\n",
       " ('<groupId>commons-logging</groupId>', 1),\n",
       " ('<artifactId>jline</artifactId>', 1),\n",
       " ('<artifactId>avro</artifactId>', 1),\n",
       " ('<artifactId>libthrift</artifactId>', 1),\n",
       " ('<groupId>com.github.scopt</groupId>', 1),\n",
       " ('<artifactId>scopt_${scala.binary.version}</artifactId>', 1),\n",
       " ('<version>3.2.0</version>', 1),\n",
       " ('following', 1),\n",
       " ('are', 1),\n",
       " ('already', 1),\n",
       " ('Spark', 1),\n",
       " ('assembly,', 1),\n",
       " ('we', 1),\n",
       " ('them', 1),\n",
       " ('provided.', 1),\n",
       " ('</dependencies>', 2),\n",
       " ('<build>', 1),\n",
       " ('<outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>',\n",
       "  1),\n",
       " ('<plugin>', 3),\n",
       " ('<groupId>org.apache.maven.plugins</groupId>', 3),\n",
       " ('<configuration>', 3),\n",
       " ('</plugin>', 3),\n",
       " ('<artifactId>maven-install-plugin</artifactId>', 1),\n",
       " ('<includes>', 1),\n",
       " ('</artifactSet>', 1),\n",
       " ('<filter>', 1),\n",
       " ('<exclude>META-INF/*.SF</exclude>', 1),\n",
       " ('<exclude>META-INF/*.RSA</exclude>', 1),\n",
       " ('</filter>', 1),\n",
       " ('<transformers>', 1),\n",
       " ('/>', 1),\n",
       " ('</transformer>', 2),\n",
       " ('implementation=\"org.apache.maven.plugins.shade.resource.DontIncludeResourceTransformer\">',\n",
       "  1),\n",
       " ('<resource>log4j.properties</resource>', 1),\n",
       " ('</transformers>', 1),\n",
       " ('<artifactId>spark-streaming-kinesis-asl_${scala.binary.version}</artifactId>',\n",
       "  1),\n",
       " ('</profile>', 6),\n",
       " ('Profiles', 1),\n",
       " ('disable', 1),\n",
       " ('inclusion', 1),\n",
       " ('certain', 1),\n",
       " ('dependencies.', 1),\n",
       " ('<flume.deps.scope>provided</flume.deps.scope>', 1),\n",
       " ('<id>hbase-provided</id>', 1),\n",
       " ('<hbase.deps.scope>provided</hbase.deps.scope>', 1),\n",
       " ('<id>parquet-provided</id>', 1),\n",
       " ('<parquet.deps.scope>provided</parquet.deps.scope>', 1),\n",
       " ('encoding=\"UTF-8\"?>', 1),\n",
       " ('<!--', 8),\n",
       " ('~', 14),\n",
       " ('Licensed', 1),\n",
       " ('to', 5),\n",
       " ('the', 10),\n",
       " ('Software', 1),\n",
       " ('Foundation', 1),\n",
       " ('(ASF)', 1),\n",
       " ('under', 4),\n",
       " ('one', 1),\n",
       " ('or', 3),\n",
       " ('contributor', 1),\n",
       " ('license', 1),\n",
       " ('agreements.', 1),\n",
       " ('See', 2),\n",
       " ('file', 3),\n",
       " ('distributed', 3),\n",
       " ('with', 2),\n",
       " ('for', 2),\n",
       " ('information', 1),\n",
       " ('ownership.', 1),\n",
       " ('ASF', 1),\n",
       " ('licenses', 1),\n",
       " ('You', 2),\n",
       " ('License,', 1),\n",
       " ('Version', 1),\n",
       " ('you', 1),\n",
       " ('not', 1),\n",
       " ('except', 1),\n",
       " ('a', 1),\n",
       " ('copy', 1),\n",
       " ('License', 3),\n",
       " ('http://www.apache.org/licenses/LICENSE-2.0', 1),\n",
       " ('Unless', 1),\n",
       " ('required', 1),\n",
       " ('by', 1),\n",
       " ('applicable', 1),\n",
       " ('agreed', 1),\n",
       " ('writing,', 1),\n",
       " ('software', 1),\n",
       " ('on', 1),\n",
       " ('WITHOUT', 1),\n",
       " ('WARRANTIES', 1),\n",
       " ('OR', 1),\n",
       " ('ANY', 1),\n",
       " ('either', 1),\n",
       " ('express', 1),\n",
       " ('implied.', 1),\n",
       " ('governing', 1),\n",
       " ('permissions', 1),\n",
       " ('and', 1),\n",
       " ('<project', 1),\n",
       " ('xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"', 1),\n",
       " ('xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0', 1),\n",
       " ('<parent>', 1),\n",
       " ('<groupId>org.apache.spark</groupId>', 14),\n",
       " ('<version>1.6.0-SNAPSHOT</version>', 1),\n",
       " ('<artifactId>spark-examples_2.10</artifactId>', 1),\n",
       " ('<properties>', 6),\n",
       " ('</properties>', 6),\n",
       " ('<packaging>jar</packaging>', 1),\n",
       " ('<name>Spark', 1),\n",
       " ('<url>http://spark.apache.org/</url>', 1),\n",
       " ('<dependency>', 25),\n",
       " ('<artifactId>spark-core_${scala.binary.version}</artifactId>', 1),\n",
       " ('<scope>provided</scope>', 8),\n",
       " ('</dependency>', 25),\n",
       " ('<artifactId>spark-mllib_${scala.binary.version}</artifactId>', 1),\n",
       " ('<artifactId>spark-streaming-twitter_${scala.binary.version}</artifactId>',\n",
       "  1),\n",
       " ('<artifactId>spark-streaming-mqtt_${scala.binary.version}</artifactId>', 1),\n",
       " ('<artifactId>spark-streaming-zeromq_${scala.binary.version}</artifactId>',\n",
       "  1),\n",
       " ('<artifactId>protobuf-java</artifactId>', 1),\n",
       " ('</exclusions>', 6),\n",
       " ('<artifactId>spark-streaming-kafka_${scala.binary.version}</artifactId>', 1),\n",
       " ('<groupId>org.apache.hbase</groupId>', 12),\n",
       " ('<artifactId>hbase-testing-util</artifactId>', 1),\n",
       " ('<version>${hbase.version}</version>', 7),\n",
       " ('<scope>${hbase.deps.scope}</scope>', 6),\n",
       " ('<artifactId>hbase-annotations</artifactId>', 4),\n",
       " ('<groupId>org.jruby</groupId>', 1),\n",
       " ('<artifactId>jruby-complete</artifactId>', 1),\n",
       " ('<artifactId>hbase-client</artifactId>', 1),\n",
       " ('<artifactId>hbase-server</artifactId>', 1),\n",
       " ('<artifactId>hadoop-mapreduce-client-core</artifactId>', 1),\n",
       " ('<artifactId>hadoop-annotations</artifactId>', 1),\n",
       " ('<artifactId>hadoop-hdfs</artifactId>', 1),\n",
       " ('<groupId>org.apache.commons</groupId>', 3),\n",
       " ('<groupId>com.sun.jersey</groupId>', 4),\n",
       " ('<artifactId>slf4j-api</artifactId>', 1),\n",
       " ('hbase', 1),\n",
       " ('v2.4,', 1),\n",
       " ('which', 1),\n",
       " ('...-->', 1),\n",
       " ('<artifactId>hbase-hadoop-compat</artifactId>', 2),\n",
       " ('<type>test-jar</type>', 1),\n",
       " ('<artifactId>algebird-core_${scala.binary.version}</artifactId>', 1),\n",
       " ('<version>0.9.0</version>', 1),\n",
       " ('<artifactId>scalacheck_${scala.binary.version}</artifactId>', 1),\n",
       " ('<groupId>org.apache.cassandra</groupId>', 1),\n",
       " ('<groupId>com.google.guava</groupId>', 1),\n",
       " ('<groupId>com.ning</groupId>', 1),\n",
       " ('<artifactId>compress-lzf</artifactId>', 1),\n",
       " ('<groupId>commons-cli</groupId>', 1),\n",
       " ('<groupId>commons-codec</groupId>', 1),\n",
       " ('<artifactId>commons-codec</artifactId>', 1),\n",
       " ('<groupId>commons-lang</groupId>', 1),\n",
       " ('<artifactId>commons-lang</artifactId>', 1),\n",
       " ('<artifactId>commons-logging</artifactId>', 1),\n",
       " ('<groupId>jline</groupId>', 1),\n",
       " ('<groupId>net.jpountz.lz4</groupId>', 1),\n",
       " ('<artifactId>lz4</artifactId>', 1),\n",
       " ('<groupId>org.apache.cassandra.deps</groupId>', 1),\n",
       " ('<groupId>org.apache.thrift</groupId>', 1),\n",
       " ('dependencies', 1),\n",
       " ('present', 1),\n",
       " ('so', 1),\n",
       " ('want', 1),\n",
       " ('force', 1),\n",
       " ('be', 1),\n",
       " ('<groupId>org.scala-lang</groupId>', 1),\n",
       " ('<artifactId>scala-library</artifactId>', 1),\n",
       " ('<testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>',\n",
       "  1),\n",
       " ('<plugins>', 1),\n",
       " ('<artifactId>maven-deploy-plugin</artifactId>', 1),\n",
       " ('<skip>true</skip>', 2),\n",
       " ('</configuration>', 3),\n",
       " ('<artifactId>maven-shade-plugin</artifactId>', 1),\n",
       " ('<shadedArtifactAttached>false</shadedArtifactAttached>', 1),\n",
       " ('<outputFile>${project.build.directory}/scala-${scala.binary.version}/spark-examples-${project.version}-hadoop${hadoop.version}.jar</outputFile>',\n",
       "  1),\n",
       " ('<artifactSet>', 1),\n",
       " ('<include>*:*</include>', 1),\n",
       " ('</includes>', 1),\n",
       " ('<filters>', 1),\n",
       " ('<artifact>*:*</artifact>', 1),\n",
       " ('<excludes>', 1),\n",
       " ('<exclude>META-INF/*.DSA</exclude>', 1),\n",
       " ('</excludes>', 1),\n",
       " ('</filters>', 1),\n",
       " ('<transformer', 3),\n",
       " ('implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"',\n",
       "  1),\n",
       " ('implementation=\"org.apache.maven.plugins.shade.resource.AppendingTransformer\">',\n",
       "  1),\n",
       " ('<resource>reference.conf</resource>', 1),\n",
       " ('</plugins>', 1),\n",
       " ('</build>', 1),\n",
       " ('<profiles>', 1),\n",
       " ('<profile>', 6),\n",
       " ('<id>kinesis-asl</id>', 1),\n",
       " ('that', 1),\n",
       " ('<id>flume-provided</id>', 1),\n",
       " ('<id>hadoop-provided</id>', 1),\n",
       " ('<hadoop.deps.scope>provided</hadoop.deps.scope>', 1),\n",
       " ('<id>hive-provided</id>', 1),\n",
       " ('<hive.deps.scope>provided</hive.deps.scope>', 1),\n",
       " ('</profiles>', 1),\n",
       " ('</project>', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountPOM.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pom Count\n",
      "\n",
      "[('<?xml', 1), ('version=\"1.0\"', 1), ('Apache', 2), ('more', 1), ('NOTICE', 1), ('this', 3), ('work', 1), ('additional', 1), ('regarding', 1), ('copyright', 1), ('The', 2), ('2.0', 1), ('(the', 1), ('\"License\");', 1), ('may', 2), ('use', 1), ('in', 3), ('compliance', 1), ('License.', 2), ('obtain', 1), ('of', 2), ('at', 1), ('law', 1), ('is', 2), ('an', 1), ('\"AS', 1), ('IS\"', 1), ('BASIS,', 1), ('CONDITIONS', 1), ('OF', 1), ('KIND,', 1), ('specific', 1), ('language', 1), ('limitations', 1), ('-->', 7), ('xmlns=\"http://maven.apache.org/POM/4.0.0\"', 1), ('http://maven.apache.org/xsd/maven-4.0.0.xsd\">', 1), ('<modelVersion>4.0.0</modelVersion>', 1), ('<artifactId>spark-parent_2.10</artifactId>', 1), ('<relativePath>../pom.xml</relativePath>', 1), ('</parent>', 1), ('<sbt.project.name>examples</sbt.project.name>', 1), ('Project', 1), ('Examples</name>', 1), ('<dependencies>', 2), ('<version>${project.version}</version>', 12), ('<artifactId>spark-streaming_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-bagel_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-hive_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-graphx_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-streaming-flume_${scala.binary.version}</artifactId>', 1), ('<exclusions>', 6), ('<exclusion>', 35), ('<groupId>org.spark-project.protobuf</groupId>', 1), ('</exclusion>', 35), ('SPARK-4455', 4), ('<artifactId>hbase-protocol</artifactId>', 1), ('<artifactId>hbase-common</artifactId>', 1), ('<groupId>io.netty</groupId>', 2), ('<artifactId>netty</artifactId>', 2), ('<groupId>org.apache.hadoop</groupId>', 7), ('<artifactId>hadoop-core</artifactId>', 1), ('<artifactId>hadoop-client</artifactId>', 1), ('<artifactId>hadoop-mapreduce-client-jobclient</artifactId>', 1), ('<artifactId>hadoop-auth</artifactId>', 1), ('<artifactId>hbase-hadoop1-compat</artifactId>', 1), ('<artifactId>commons-math</artifactId>', 1), ('<artifactId>jersey-core</artifactId>', 2), ('<groupId>org.slf4j</groupId>', 1), ('<artifactId>jersey-server</artifactId>', 1), ('<artifactId>jersey-json</artifactId>', 1), ('uses', 1), ('better,', 1), ('but', 1), ('<groupId>commons-io</groupId>', 1), ('<artifactId>commons-io</artifactId>', 1), ('<scope>test</scope>', 2), ('<artifactId>commons-math3</artifactId>', 2), ('<groupId>com.twitter</groupId>', 1), ('<groupId>org.scalacheck</groupId>', 1), ('<artifactId>cassandra-all</artifactId>', 1), ('<version>1.2.6</version>', 1), ('<artifactId>guava</artifactId>', 1), ('<groupId>com.googlecode.concurrentlinkedhashmap</groupId>', 1), ('<artifactId>concurrentlinkedhashmap-lru</artifactId>', 1), ('<artifactId>commons-cli</artifactId>', 1), ('<groupId>commons-logging</groupId>', 1), ('<artifactId>jline</artifactId>', 1), ('<artifactId>avro</artifactId>', 1), ('<artifactId>libthrift</artifactId>', 1), ('<groupId>com.github.scopt</groupId>', 1), ('<artifactId>scopt_${scala.binary.version}</artifactId>', 1), ('<version>3.2.0</version>', 1), ('following', 1), ('are', 1), ('already', 1), ('Spark', 1), ('assembly,', 1), ('we', 1), ('them', 1), ('provided.', 1), ('</dependencies>', 2), ('<build>', 1), ('<outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>', 1), ('<plugin>', 3), ('<groupId>org.apache.maven.plugins</groupId>', 3), ('<configuration>', 3), ('</plugin>', 3), ('<artifactId>maven-install-plugin</artifactId>', 1), ('<includes>', 1), ('</artifactSet>', 1), ('<filter>', 1), ('<exclude>META-INF/*.SF</exclude>', 1), ('<exclude>META-INF/*.RSA</exclude>', 1), ('</filter>', 1), ('<transformers>', 1), ('/>', 1), ('</transformer>', 2), ('implementation=\"org.apache.maven.plugins.shade.resource.DontIncludeResourceTransformer\">', 1), ('<resource>log4j.properties</resource>', 1), ('</transformers>', 1), ('<artifactId>spark-streaming-kinesis-asl_${scala.binary.version}</artifactId>', 1), ('</profile>', 6), ('Profiles', 1), ('disable', 1), ('inclusion', 1), ('certain', 1), ('dependencies.', 1), ('<flume.deps.scope>provided</flume.deps.scope>', 1), ('<id>hbase-provided</id>', 1), ('<hbase.deps.scope>provided</hbase.deps.scope>', 1), ('<id>parquet-provided</id>', 1), ('<parquet.deps.scope>provided</parquet.deps.scope>', 1), ('encoding=\"UTF-8\"?>', 1), ('<!--', 8), ('~', 14), ('Licensed', 1), ('to', 5), ('the', 10), ('Software', 1), ('Foundation', 1), ('(ASF)', 1), ('under', 4), ('one', 1), ('or', 3), ('contributor', 1), ('license', 1), ('agreements.', 1), ('See', 2), ('file', 3), ('distributed', 3), ('with', 2), ('for', 2), ('information', 1), ('ownership.', 1), ('ASF', 1), ('licenses', 1), ('You', 2), ('License,', 1), ('Version', 1), ('you', 1), ('not', 1), ('except', 1), ('a', 1), ('copy', 1), ('License', 3), ('http://www.apache.org/licenses/LICENSE-2.0', 1), ('Unless', 1), ('required', 1), ('by', 1), ('applicable', 1), ('agreed', 1), ('writing,', 1), ('software', 1), ('on', 1), ('WITHOUT', 1), ('WARRANTIES', 1), ('OR', 1), ('ANY', 1), ('either', 1), ('express', 1), ('implied.', 1), ('governing', 1), ('permissions', 1), ('and', 1), ('<project', 1), ('xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"', 1), ('xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0', 1), ('<parent>', 1), ('<groupId>org.apache.spark</groupId>', 14), ('<version>1.6.0-SNAPSHOT</version>', 1), ('<artifactId>spark-examples_2.10</artifactId>', 1), ('<properties>', 6), ('</properties>', 6), ('<packaging>jar</packaging>', 1), ('<name>Spark', 1), ('<url>http://spark.apache.org/</url>', 1), ('<dependency>', 25), ('<artifactId>spark-core_${scala.binary.version}</artifactId>', 1), ('<scope>provided</scope>', 8), ('</dependency>', 25), ('<artifactId>spark-mllib_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-streaming-twitter_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-streaming-mqtt_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-streaming-zeromq_${scala.binary.version}</artifactId>', 1), ('<artifactId>protobuf-java</artifactId>', 1), ('</exclusions>', 6), ('<artifactId>spark-streaming-kafka_${scala.binary.version}</artifactId>', 1), ('<groupId>org.apache.hbase</groupId>', 12), ('<artifactId>hbase-testing-util</artifactId>', 1), ('<version>${hbase.version}</version>', 7), ('<scope>${hbase.deps.scope}</scope>', 6), ('<artifactId>hbase-annotations</artifactId>', 4), ('<groupId>org.jruby</groupId>', 1), ('<artifactId>jruby-complete</artifactId>', 1), ('<artifactId>hbase-client</artifactId>', 1), ('<artifactId>hbase-server</artifactId>', 1), ('<artifactId>hadoop-mapreduce-client-core</artifactId>', 1), ('<artifactId>hadoop-annotations</artifactId>', 1), ('<artifactId>hadoop-hdfs</artifactId>', 1), ('<groupId>org.apache.commons</groupId>', 3), ('<groupId>com.sun.jersey</groupId>', 4), ('<artifactId>slf4j-api</artifactId>', 1), ('hbase', 1), ('v2.4,', 1), ('which', 1), ('...-->', 1), ('<artifactId>hbase-hadoop-compat</artifactId>', 2), ('<type>test-jar</type>', 1), ('<artifactId>algebird-core_${scala.binary.version}</artifactId>', 1), ('<version>0.9.0</version>', 1), ('<artifactId>scalacheck_${scala.binary.version}</artifactId>', 1), ('<groupId>org.apache.cassandra</groupId>', 1), ('<groupId>com.google.guava</groupId>', 1), ('<groupId>com.ning</groupId>', 1), ('<artifactId>compress-lzf</artifactId>', 1), ('<groupId>commons-cli</groupId>', 1), ('<groupId>commons-codec</groupId>', 1), ('<artifactId>commons-codec</artifactId>', 1), ('<groupId>commons-lang</groupId>', 1), ('<artifactId>commons-lang</artifactId>', 1), ('<artifactId>commons-logging</artifactId>', 1), ('<groupId>jline</groupId>', 1), ('<groupId>net.jpountz.lz4</groupId>', 1), ('<artifactId>lz4</artifactId>', 1), ('<groupId>org.apache.cassandra.deps</groupId>', 1), ('<groupId>org.apache.thrift</groupId>', 1), ('dependencies', 1), ('present', 1), ('so', 1), ('want', 1), ('force', 1), ('be', 1), ('<groupId>org.scala-lang</groupId>', 1), ('<artifactId>scala-library</artifactId>', 1), ('<testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>', 1), ('<plugins>', 1), ('<artifactId>maven-deploy-plugin</artifactId>', 1), ('<skip>true</skip>', 2), ('</configuration>', 3), ('<artifactId>maven-shade-plugin</artifactId>', 1), ('<shadedArtifactAttached>false</shadedArtifactAttached>', 1), ('<outputFile>${project.build.directory}/scala-${scala.binary.version}/spark-examples-${project.version}-hadoop${hadoop.version}.jar</outputFile>', 1), ('<artifactSet>', 1), ('<include>*:*</include>', 1), ('</includes>', 1), ('<filters>', 1), ('<artifact>*:*</artifact>', 1), ('<excludes>', 1), ('<exclude>META-INF/*.DSA</exclude>', 1), ('</excludes>', 1), ('</filters>', 1), ('<transformer', 3), ('implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"', 1), ('implementation=\"org.apache.maven.plugins.shade.resource.AppendingTransformer\">', 1), ('<resource>reference.conf</resource>', 1), ('</plugins>', 1), ('</build>', 1), ('<profiles>', 1), ('<profile>', 6), ('<id>kinesis-asl</id>', 1), ('that', 1), ('<id>flume-provided</id>', 1), ('<id>hadoop-provided</id>', 1), ('<hadoop.deps.scope>provided</hadoop.deps.scope>', 1), ('<id>hive-provided</id>', 1), ('<hive.deps.scope>provided</hive.deps.scope>', 1), ('</profiles>', 1), ('</project>', 1)]\n"
     ]
    }
   ],
   "source": [
    "# or\n",
    "print(\"Pom Count\\n\")\n",
    "print(wordCountPOM.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *The join function combines the two datasets (K,V) and (K,W) together and get (K, (V,W)). Let's join these two counts together.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join function \n",
    "joined = wordCountREADME.join(wordCountPOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', (1, 2)),\n",
       " ('Spark', (14, 1)),\n",
       " ('is', (6, 2)),\n",
       " ('in', (5, 3)),\n",
       " ('an', (3, 1)),\n",
       " ('of', (5, 2)),\n",
       " ('this', (1, 3)),\n",
       " ('at', (2, 1)),\n",
       " ('The', (1, 2)),\n",
       " ('following', (2, 1)),\n",
       " ('use', (3, 1)),\n",
       " ('are', (1, 1)),\n",
       " ('uses', (1, 1)),\n",
       " ('a', (10, 1)),\n",
       " ('and', (10, 1)),\n",
       " ('for', (12, 2)),\n",
       " ('that', (3, 1)),\n",
       " ('You', (3, 2)),\n",
       " ('the', (21, 10)),\n",
       " ('on', (6, 1)),\n",
       " ('file', (1, 3)),\n",
       " ('not', (1, 1)),\n",
       " ('to', (14, 5)),\n",
       " ('you', (4, 1)),\n",
       " ('which', (2, 1)),\n",
       " ('with', (4, 2)),\n",
       " ('one', (2, 1)),\n",
       " ('be', (2, 1)),\n",
       " ('or', (3, 3)),\n",
       " ('See', (1, 2))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's combine the values together to get the total count\n",
    "joinedSum = joined.map(lambda k: (k[0], (k[1][0]+k[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined Individial\n",
      "\n",
      "[('Apache', (1, 2)), ('Spark', (14, 1)), ('is', (6, 2)), ('in', (5, 3)), ('an', (3, 1))]\n",
      "\n",
      "\n",
      "Joined Sum\n",
      "\n",
      "[('Apache', 3), ('Spark', 15), ('is', 8), ('in', 8), ('an', 4)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Joined Individial\\n\")\n",
    "print(joined.take(5))\n",
    "\n",
    "print(\"\\n\\nJoined Sum\\n\")\n",
    "print(joinedSum.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Shared variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, when a function passed to a Spark operation (such as map or reduce) is executed on a remote cluster node, it works on separate copies of all the variables used in the function. These variables are copied to each machine, and no updates to the variables on the remote machine are propagated back to the driver program. Supporting general, read-write shared variables across tasks would be inefficient. However, Spark does provide two limited types of shared variables for two common usage patterns: broadcast variables and accumulators.\n",
    "\n",
    "Normalmente, quando uma função passada para uma operação do Spark (como mapear ou reduzir) é executada em um nó de cluster remoto, ela funciona em cópias separadas de todas as variáveis usadas na função. Essas variáveis são copiadas para cada máquina, e nenhuma atualização das variáveis na máquina remota é propagada de volta para o programa do driver. O suporte a variáveis compartilhadas gerais de leitura e gravação entre as tarefas seria ineficiente. No entanto, o Spark fornece dois tipos limitados de variáveis compartilhadas para dois padrões de uso comuns: variáveis de transmissão e acumuladores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast variables\n",
    "\n",
    "Broadcast variables are useful for when you have a large dataset that you want to use across all the worker nodes. A read-only variable is cached on each machine rather than shipping a copy of it with tasks. Spark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage.\n",
    "\n",
    "More here: http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables\n",
    "\n",
    "\n",
    "Variáveis de transmissão são úteis quando você tem um grande conjunto de dados que deseja usar em todos os nós de trabalho. Uma variável somente leitura é armazenada em cache em cada máquina, em vez de enviar uma cópia dela com as tarefas. As ações do Spark são executadas por meio de um conjunto de estágios, separados por operações \"shuffle\" distribuídas. O Spark transmite automaticamente os dados comuns necessários para as tarefas em cada está"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a broadcast variable\n",
    "broadcastVar = sc.broadcast([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators\n",
    "\n",
    "Accumulators are variables that can only be added through an associative operation. It is used to implement counters and sum efficiently in parallel. Spark natively supports numeric type accumulators and standard mutable collections. Programmers can extend these for new types. Only the driver can read the values of the accumulators. The workers can only invoke it to increment the value.\n",
    "\n",
    "Acumuladores são variáveis que só podem ser adicionadas por meio de uma operação associativa. É usado para implementar contadores e somar de forma eficiente em paralelo. O Spark oferece suporte nativo a acumuladores de tipo numérico e coleções mutáveis padrão. Os programadores podem estendê-los para novos tipos. Apenas o driver pode ler os valores dos acumuladores. Os workers só podem invocá-lo para incrementar o valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the accumulator variable\n",
    "accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next parallelize an array of four integers and run it through a loop to add each integer value to \n",
    "# the accumulator variable\n",
    "rdd = sc.parallelize([1,2,3,4])\n",
    "def f(x):\n",
    "    global accum \n",
    "    accum += x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, iterate through each element of the rdd and apply the function f on it:\n",
    "rdd.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the current value of the accumulator variable, type in:\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command can only be invoked on the driver side. The worker nodes can only increment the accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a key-value pair of two characters\n",
    "pair = ('a', 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "# To access the value of the first index use [0] and [1] method for the 2nd.\n",
    "print(pair[0])\n",
    "print(pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
